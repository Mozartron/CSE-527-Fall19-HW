{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"name":"Ramajayam_Gautham_112965275_hw5.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AcamCuuyjYb2"},"source":["# Action Recognition @ UCF101  \n","**Due date: 11:59 pm on Nov. 19, 2019 (Tuesday)**\n","\n","## Description\n","---\n","In this homework, you will be doing action recognition using Recurrent Neural Network (RNN), (Long-Short Term Memory) LSTM in particular. You will be given a dataset called UCF101, which consists of 101 different actions/classes and for each action, there will be 145 samples. We tagged each sample into either training or testing. Each sample is supposed to be a short video, but we sampled 25 frames from each videos to reduce the amount of data. Consequently, a training sample is an image tuple that forms a 3D volume with one dimension encoding *temporal correlation* between frames and a label indicating what action it is.\n","\n","To tackle this problem, we aim to build a neural network that can not only capture spatial information of each frame but also temporal information between frames. Fortunately, you don't have to do this on your own. RNN — a type of neural network designed to deal with time-series data — is right here for you to use. In particular, you will be using LSTM for this task.\n","\n","Instead of training an end-to-end neural network from scratch whose computation is prohibitively expensive, we divide this into two steps: feature extraction and modelling. Below are the things you need to implement for this homework:\n","- **{35 pts} Feature extraction**. Use any of the [pre-trained models](https://pytorch.org/docs/stable/torchvision/models.html) to extract features from each frame. Specifically, we recommend not to use the activations of the last layer as the features tend to be task specific towards the end of the network. \n","    **hints**: \n","    - A good starting point would be to use a pre-trained VGG16 network, we suggest first fully connected layer `torchvision.models.vgg16` (4096 dim) as features of each video frame. This will result into a 4096x25 matrix for each video. \n","    - Normalize your images using `torchvision.transforms` \n","    ```\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n","    prep(img)\n","    The mean and std. mentioned above is specific to Imagenet data\n","    \n","    ```\n","    More details of image preprocessing in PyTorch can be found at http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n","    \n","- **{35 pts} Modelling**. With the extracted features, build an LSTM network which takes a **dx25** sample as input (where **d** is the dimension of the extracted feature for each frame), and outputs the action label of that sample.\n","- **{20 pts} Evaluation**. After training your network, you need to evaluate your model with the testing data by computing the prediction accuracy **(5 points)**. The baseline test accuracy for this data is 75%, and **10 points** out of 20 is for achieving test accuracy greater than the baseline. Moreover, you need to compare **(5 points)** the result of your network with that of support vector machine (SVM) (stacking the **dx25** feature matrix to a long vector and train a SVM).\n","- **{10 pts} Report**. Details regarding the report can be found in the submission section below.\n","\n","Notice that the size of the raw images is 256x340, whereas your pre-trained model might take **nxn** images as inputs. To solve this problem, instead of resizing the images which unfavorably changes the spatial ratio, we take a better solution: Cropping five **nxn** images, one at the image center and four at the corners and compute the **d**-dim features for each of them, and average these five **d**-dim feature to get a final feature representation for the raw image.\n","For example, VGG takes 224x224 images as inputs, so we take the five 224x224 croppings of the image, compute 4096-dim VGG features for each of them, and then take the mean of these five 4096-dim vectors to be the representation of the image.\n","\n","In order to save you computational time, you need to do the classification task only for **the first 25** classes of the whole dataset. The same applies to those who have access to GPUs. **Bonus 10 points for running and reporting on the entire 101 classes.**\n","\n","\n","## Dataset\n","Download **dataset** at [UCF101](http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar)(Image data for each video) and the **annos folder** which has the video labels and the label to class name mapping is included in the assignment folder uploaded. \n","\n","\n","UCF101 dataset contains 101 actions and 13,320 videos in total.  \n","\n","+ `annos/actions.txt`  \n","  + lists all the actions (`ApplyEyeMakeup`, .., `YoYo`)   \n","  \n","+ `annots/videos_labels_subsets.txt`  \n","  + lists all the videos (`v_000001`, .., `v_013320`)  \n","  + labels (`1`, .., `101`)  \n","  + subsets (`1` for train, `2` for test)  \n","\n","+ `images/`  \n","  + each folder represents a video\n","  + the video/folder name to class mapping can be found using `annots/videos_labels_subsets.txt`, for e.g. `v_000001` belongs to class 1 i.e. `ApplyEyeMakeup`\n","  + each video folder contains 25 frames  \n","\n","\n","\n","## Some Tutorials\n","- Good materials for understanding RNN and LSTM\n","    - http://blog.echen.me\n","    - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n","    - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","- Implementing RNN and LSTM with PyTorch\n","    - [LSTM with PyTorch](http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py)\n","    - [RNN with PyTorch](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UdCRwkL7jxtc"},"source":["---\n","---\n","## **Problem 1.** Feature extraction"]},{"cell_type":"code","metadata":{"id":"SJMk9aUrk-t9","colab_type":"code","colab":{}},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dQn4F99RO63","colab_type":"code","outputId":"cc102aeb-b477-4612-b328-910c5fa642d8","executionInfo":{"status":"ok","timestamp":1574479006395,"user_tz":300,"elapsed":26272,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6JfYsnF-sCbO","colab_type":"code","outputId":"91405e22-caf1-468d-e786-135c485a73d1","executionInfo":{"status":"ok","timestamp":1574234586854,"user_tz":300,"elapsed":211,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pwd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/home/gautham/Desktop/cv_hw5/UCF101_images\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EZTNwngx8eUW","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/gdrive/My Drive/Ramajayam_Gautham_112965275_hw5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDyH7HHFvqMp","colab_type":"code","outputId":"8014aa42-5665-4e2d-b37c-b95a819377d2","executionInfo":{"status":"ok","timestamp":1574350427243,"user_tz":300,"elapsed":892,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#!conda install pytorch=1.0.1 -c pytorch\n","import torch \n","print(torch.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0pmMXpKcRP0S","colab_type":"code","colab":{}},"source":["#import os\n","import tarfile\n","import time\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","\n","import numpy as np\n","import scipy as sp\n","import cv2\n","import glob\n","import pdb\n","\n","\n","#tar = tarfile.open('./data/UCF101_images.tar',\"r:\")\n","#tar.extractall()\n","#tar.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rt0td1CZ07T","colab_type":"code","outputId":"ca484b0a-950a-46e8-d6a7-23465946c118","executionInfo":{"status":"ok","timestamp":1574479017726,"user_tz":300,"elapsed":1269,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["df = pd.read_csv('./annos/videos_labels_subsets.txt', sep=\"\\t\", header=None, names=[\"folder\", \"class_id\", \"data_usage\"])\n","#df.head()\n","classes = pd.read_csv('./annos/actions.txt', sep=\"\\t\", header = None)\n","\n","df_small = df[df.class_id <= 25]\n","\n","df_small_train = df_small[df_small.data_usage == 1]\n","df_small_test = df_small[df_small.data_usage == 2]\n","\n","train25_folders = df_small_train.folder.tolist()\n","train25_labels = df_small_train.class_id.tolist()\n","\n","test25_folders = df_small_test.folder.tolist()\n","test25_labels = df_small_test.class_id.tolist()\n","#classes\n","\n","df_train = df[df.data_usage == 1]\n","df_test = df[df.data_usage == 2]\n","\n","train_folders = df_train.folder.tolist()\n","train_labels = df_train.class_id.tolist()\n","\n","test_folders = df_test.folder.tolist()\n","test_labels = df_test.class_id.tolist()\n","\n","print(train25_folders)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['v_000045', 'v_000046', 'v_000047', 'v_000048', 'v_000049', 'v_000050', 'v_000051', 'v_000052', 'v_000053', 'v_000054', 'v_000055', 'v_000056', 'v_000057', 'v_000058', 'v_000059', 'v_000060', 'v_000061', 'v_000062', 'v_000063', 'v_000064', 'v_000065', 'v_000066', 'v_000067', 'v_000068', 'v_000069', 'v_000070', 'v_000071', 'v_000072', 'v_000073', 'v_000074', 'v_000075', 'v_000076', 'v_000077', 'v_000078', 'v_000079', 'v_000080', 'v_000081', 'v_000082', 'v_000083', 'v_000084', 'v_000085', 'v_000086', 'v_000087', 'v_000088', 'v_000089', 'v_000090', 'v_000091', 'v_000092', 'v_000093', 'v_000094', 'v_000095', 'v_000096', 'v_000097', 'v_000098', 'v_000099', 'v_000100', 'v_000101', 'v_000102', 'v_000103', 'v_000104', 'v_000105', 'v_000106', 'v_000107', 'v_000108', 'v_000109', 'v_000110', 'v_000111', 'v_000112', 'v_000113', 'v_000114', 'v_000115', 'v_000116', 'v_000117', 'v_000118', 'v_000119', 'v_000120', 'v_000121', 'v_000122', 'v_000123', 'v_000124', 'v_000125', 'v_000126', 'v_000127', 'v_000128', 'v_000129', 'v_000130', 'v_000131', 'v_000132', 'v_000133', 'v_000134', 'v_000135', 'v_000136', 'v_000137', 'v_000138', 'v_000139', 'v_000140', 'v_000141', 'v_000142', 'v_000143', 'v_000144', 'v_000145', 'v_000178', 'v_000179', 'v_000180', 'v_000181', 'v_000182', 'v_000183', 'v_000184', 'v_000185', 'v_000186', 'v_000187', 'v_000188', 'v_000189', 'v_000190', 'v_000191', 'v_000192', 'v_000193', 'v_000194', 'v_000195', 'v_000196', 'v_000197', 'v_000198', 'v_000199', 'v_000200', 'v_000201', 'v_000202', 'v_000203', 'v_000204', 'v_000205', 'v_000206', 'v_000207', 'v_000208', 'v_000209', 'v_000210', 'v_000211', 'v_000212', 'v_000213', 'v_000214', 'v_000215', 'v_000216', 'v_000217', 'v_000218', 'v_000219', 'v_000220', 'v_000221', 'v_000222', 'v_000223', 'v_000224', 'v_000225', 'v_000226', 'v_000227', 'v_000228', 'v_000229', 'v_000230', 'v_000231', 'v_000232', 'v_000233', 'v_000234', 'v_000235', 'v_000236', 'v_000237', 'v_000238', 'v_000239', 'v_000240', 'v_000241', 'v_000242', 'v_000243', 'v_000244', 'v_000245', 'v_000246', 'v_000247', 'v_000248', 'v_000249', 'v_000250', 'v_000251', 'v_000252', 'v_000253', 'v_000254', 'v_000255', 'v_000256', 'v_000257', 'v_000258', 'v_000259', 'v_000301', 'v_000302', 'v_000303', 'v_000304', 'v_000305', 'v_000306', 'v_000307', 'v_000308', 'v_000309', 'v_000310', 'v_000311', 'v_000312', 'v_000313', 'v_000314', 'v_000315', 'v_000316', 'v_000317', 'v_000318', 'v_000319', 'v_000320', 'v_000321', 'v_000322', 'v_000323', 'v_000324', 'v_000325', 'v_000326', 'v_000327', 'v_000328', 'v_000329', 'v_000330', 'v_000331', 'v_000332', 'v_000333', 'v_000334', 'v_000335', 'v_000336', 'v_000337', 'v_000338', 'v_000339', 'v_000340', 'v_000341', 'v_000342', 'v_000343', 'v_000344', 'v_000345', 'v_000346', 'v_000347', 'v_000348', 'v_000349', 'v_000350', 'v_000351', 'v_000352', 'v_000353', 'v_000354', 'v_000355', 'v_000356', 'v_000357', 'v_000358', 'v_000359', 'v_000360', 'v_000361', 'v_000362', 'v_000363', 'v_000364', 'v_000365', 'v_000366', 'v_000367', 'v_000368', 'v_000369', 'v_000370', 'v_000371', 'v_000372', 'v_000373', 'v_000374', 'v_000375', 'v_000376', 'v_000377', 'v_000378', 'v_000379', 'v_000380', 'v_000381', 'v_000382', 'v_000383', 'v_000384', 'v_000385', 'v_000386', 'v_000387', 'v_000388', 'v_000389', 'v_000390', 'v_000391', 'v_000392', 'v_000393', 'v_000394', 'v_000395', 'v_000396', 'v_000397', 'v_000398', 'v_000399', 'v_000400', 'v_000401', 'v_000402', 'v_000403', 'v_000404', 'v_000440', 'v_000441', 'v_000442', 'v_000443', 'v_000444', 'v_000445', 'v_000446', 'v_000447', 'v_000448', 'v_000449', 'v_000450', 'v_000451', 'v_000452', 'v_000453', 'v_000454', 'v_000455', 'v_000456', 'v_000457', 'v_000458', 'v_000459', 'v_000460', 'v_000461', 'v_000462', 'v_000463', 'v_000464', 'v_000465', 'v_000466', 'v_000467', 'v_000468', 'v_000469', 'v_000470', 'v_000471', 'v_000472', 'v_000473', 'v_000474', 'v_000475', 'v_000476', 'v_000477', 'v_000478', 'v_000479', 'v_000480', 'v_000481', 'v_000482', 'v_000483', 'v_000484', 'v_000485', 'v_000486', 'v_000487', 'v_000488', 'v_000489', 'v_000490', 'v_000491', 'v_000492', 'v_000493', 'v_000494', 'v_000495', 'v_000496', 'v_000497', 'v_000498', 'v_000499', 'v_000500', 'v_000501', 'v_000502', 'v_000503', 'v_000504', 'v_000505', 'v_000506', 'v_000507', 'v_000508', 'v_000509', 'v_000510', 'v_000511', 'v_000512', 'v_000513', 'v_000514', 'v_000515', 'v_000516', 'v_000517', 'v_000518', 'v_000519', 'v_000520', 'v_000521', 'v_000522', 'v_000523', 'v_000524', 'v_000525', 'v_000526', 'v_000527', 'v_000528', 'v_000529', 'v_000530', 'v_000531', 'v_000532', 'v_000533', 'v_000534', 'v_000535', 'v_000536', 'v_000568', 'v_000569', 'v_000570', 'v_000571', 'v_000572', 'v_000573', 'v_000574', 'v_000575', 'v_000576', 'v_000577', 'v_000578', 'v_000579', 'v_000580', 'v_000581', 'v_000582', 'v_000583', 'v_000584', 'v_000585', 'v_000586', 'v_000587', 'v_000588', 'v_000589', 'v_000590', 'v_000591', 'v_000592', 'v_000593', 'v_000594', 'v_000595', 'v_000596', 'v_000597', 'v_000598', 'v_000599', 'v_000600', 'v_000601', 'v_000602', 'v_000603', 'v_000604', 'v_000605', 'v_000606', 'v_000607', 'v_000608', 'v_000609', 'v_000610', 'v_000611', 'v_000612', 'v_000613', 'v_000614', 'v_000615', 'v_000616', 'v_000617', 'v_000618', 'v_000619', 'v_000620', 'v_000621', 'v_000622', 'v_000623', 'v_000624', 'v_000625', 'v_000626', 'v_000627', 'v_000628', 'v_000629', 'v_000630', 'v_000631', 'v_000632', 'v_000633', 'v_000634', 'v_000635', 'v_000636', 'v_000637', 'v_000638', 'v_000639', 'v_000640', 'v_000641', 'v_000642', 'v_000643', 'v_000644', 'v_000688', 'v_000689', 'v_000690', 'v_000691', 'v_000692', 'v_000693', 'v_000694', 'v_000695', 'v_000696', 'v_000697', 'v_000698', 'v_000699', 'v_000700', 'v_000701', 'v_000702', 'v_000703', 'v_000704', 'v_000705', 'v_000706', 'v_000707', 'v_000708', 'v_000709', 'v_000710', 'v_000711', 'v_000712', 'v_000713', 'v_000714', 'v_000715', 'v_000716', 'v_000717', 'v_000718', 'v_000719', 'v_000720', 'v_000721', 'v_000722', 'v_000723', 'v_000724', 'v_000725', 'v_000726', 'v_000727', 'v_000728', 'v_000729', 'v_000730', 'v_000731', 'v_000732', 'v_000733', 'v_000734', 'v_000735', 'v_000736', 'v_000737', 'v_000738', 'v_000739', 'v_000740', 'v_000741', 'v_000742', 'v_000743', 'v_000744', 'v_000745', 'v_000746', 'v_000747', 'v_000748', 'v_000749', 'v_000750', 'v_000751', 'v_000752', 'v_000753', 'v_000754', 'v_000755', 'v_000756', 'v_000757', 'v_000758', 'v_000759', 'v_000760', 'v_000761', 'v_000762', 'v_000763', 'v_000764', 'v_000765', 'v_000766', 'v_000767', 'v_000768', 'v_000769', 'v_000770', 'v_000771', 'v_000772', 'v_000773', 'v_000774', 'v_000775', 'v_000776', 'v_000777', 'v_000778', 'v_000779', 'v_000780', 'v_000781', 'v_000782', 'v_000783', 'v_000784', 'v_000785', 'v_000786', 'v_000787', 'v_000788', 'v_000789', 'v_000790', 'v_000791', 'v_000792', 'v_000793', 'v_000794', 'v_000795', 'v_000796', 'v_000797', 'v_000798', 'v_000799', 'v_000843', 'v_000844', 'v_000845', 'v_000846', 'v_000847', 'v_000848', 'v_000849', 'v_000850', 'v_000851', 'v_000852', 'v_000853', 'v_000854', 'v_000855', 'v_000856', 'v_000857', 'v_000858', 'v_000859', 'v_000860', 'v_000861', 'v_000862', 'v_000863', 'v_000864', 'v_000865', 'v_000866', 'v_000867', 'v_000868', 'v_000869', 'v_000870', 'v_000871', 'v_000872', 'v_000873', 'v_000874', 'v_000875', 'v_000876', 'v_000877', 'v_000878', 'v_000879', 'v_000880', 'v_000881', 'v_000882', 'v_000883', 'v_000884', 'v_000885', 'v_000886', 'v_000887', 'v_000888', 'v_000889', 'v_000890', 'v_000891', 'v_000892', 'v_000893', 'v_000894', 'v_000895', 'v_000896', 'v_000897', 'v_000898', 'v_000899', 'v_000900', 'v_000901', 'v_000902', 'v_000903', 'v_000904', 'v_000905', 'v_000906', 'v_000907', 'v_000908', 'v_000909', 'v_000910', 'v_000911', 'v_000912', 'v_000913', 'v_000914', 'v_000915', 'v_000916', 'v_000917', 'v_000918', 'v_000919', 'v_000920', 'v_000921', 'v_000922', 'v_000923', 'v_000924', 'v_000925', 'v_000926', 'v_000927', 'v_000928', 'v_000929', 'v_000930', 'v_000931', 'v_000932', 'v_000933', 'v_000934', 'v_000935', 'v_000936', 'v_000937', 'v_000938', 'v_000939', 'v_000940', 'v_000941', 'v_000942', 'v_000943', 'v_000944', 'v_000945', 'v_000946', 'v_000947', 'v_000948', 'v_000949', 'v_000985', 'v_000986', 'v_000987', 'v_000988', 'v_000989', 'v_000990', 'v_000991', 'v_000992', 'v_000993', 'v_000994', 'v_000995', 'v_000996', 'v_000997', 'v_000998', 'v_000999', 'v_001000', 'v_001001', 'v_001002', 'v_001003', 'v_001004', 'v_001005', 'v_001006', 'v_001007', 'v_001008', 'v_001009', 'v_001010', 'v_001011', 'v_001012', 'v_001013', 'v_001014', 'v_001015', 'v_001016', 'v_001017', 'v_001018', 'v_001019', 'v_001020', 'v_001021', 'v_001022', 'v_001023', 'v_001024', 'v_001025', 'v_001026', 'v_001027', 'v_001028', 'v_001029', 'v_001030', 'v_001031', 'v_001032', 'v_001033', 'v_001034', 'v_001035', 'v_001036', 'v_001037', 'v_001038', 'v_001039', 'v_001040', 'v_001041', 'v_001042', 'v_001043', 'v_001044', 'v_001045', 'v_001046', 'v_001047', 'v_001048', 'v_001049', 'v_001050', 'v_001051', 'v_001052', 'v_001053', 'v_001054', 'v_001055', 'v_001056', 'v_001057', 'v_001058', 'v_001059', 'v_001060', 'v_001061', 'v_001062', 'v_001063', 'v_001064', 'v_001065', 'v_001066', 'v_001067', 'v_001068', 'v_001069', 'v_001070', 'v_001071', 'v_001072', 'v_001073', 'v_001074', 'v_001075', 'v_001076', 'v_001077', 'v_001078', 'v_001079', 'v_001080', 'v_001081', 'v_001082', 'v_001083', 'v_001121', 'v_001122', 'v_001123', 'v_001124', 'v_001125', 'v_001126', 'v_001127', 'v_001128', 'v_001129', 'v_001130', 'v_001131', 'v_001132', 'v_001133', 'v_001134', 'v_001135', 'v_001136', 'v_001137', 'v_001138', 'v_001139', 'v_001140', 'v_001141', 'v_001142', 'v_001143', 'v_001144', 'v_001145', 'v_001146', 'v_001147', 'v_001148', 'v_001149', 'v_001150', 'v_001151', 'v_001152', 'v_001153', 'v_001154', 'v_001155', 'v_001156', 'v_001157', 'v_001158', 'v_001159', 'v_001160', 'v_001161', 'v_001162', 'v_001163', 'v_001164', 'v_001165', 'v_001166', 'v_001167', 'v_001168', 'v_001169', 'v_001170', 'v_001171', 'v_001172', 'v_001173', 'v_001174', 'v_001175', 'v_001176', 'v_001177', 'v_001178', 'v_001179', 'v_001180', 'v_001181', 'v_001182', 'v_001183', 'v_001184', 'v_001185', 'v_001186', 'v_001187', 'v_001188', 'v_001189', 'v_001190', 'v_001191', 'v_001192', 'v_001193', 'v_001194', 'v_001195', 'v_001196', 'v_001197', 'v_001198', 'v_001199', 'v_001200', 'v_001201', 'v_001202', 'v_001203', 'v_001204', 'v_001205', 'v_001206', 'v_001207', 'v_001208', 'v_001209', 'v_001210', 'v_001211', 'v_001212', 'v_001213', 'v_001214', 'v_001263', 'v_001264', 'v_001265', 'v_001266', 'v_001267', 'v_001268', 'v_001269', 'v_001270', 'v_001271', 'v_001272', 'v_001273', 'v_001274', 'v_001275', 'v_001276', 'v_001277', 'v_001278', 'v_001279', 'v_001280', 'v_001281', 'v_001282', 'v_001283', 'v_001284', 'v_001285', 'v_001286', 'v_001287', 'v_001288', 'v_001289', 'v_001290', 'v_001291', 'v_001292', 'v_001293', 'v_001294', 'v_001295', 'v_001296', 'v_001297', 'v_001298', 'v_001299', 'v_001300', 'v_001301', 'v_001302', 'v_001303', 'v_001304', 'v_001305', 'v_001306', 'v_001307', 'v_001308', 'v_001309', 'v_001310', 'v_001311', 'v_001312', 'v_001313', 'v_001314', 'v_001315', 'v_001316', 'v_001317', 'v_001318', 'v_001319', 'v_001320', 'v_001321', 'v_001322', 'v_001323', 'v_001324', 'v_001325', 'v_001326', 'v_001327', 'v_001328', 'v_001329', 'v_001330', 'v_001331', 'v_001332', 'v_001333', 'v_001334', 'v_001335', 'v_001336', 'v_001337', 'v_001338', 'v_001339', 'v_001340', 'v_001341', 'v_001342', 'v_001343', 'v_001344', 'v_001345', 'v_001346', 'v_001347', 'v_001348', 'v_001349', 'v_001350', 'v_001351', 'v_001352', 'v_001353', 'v_001354', 'v_001355', 'v_001356', 'v_001357', 'v_001358', 'v_001359', 'v_001360', 'v_001361', 'v_001362', 'v_001363', 'v_001364', 'v_001365', 'v_001366', 'v_001367', 'v_001368', 'v_001369', 'v_001370', 'v_001371', 'v_001372', 'v_001373', 'v_001374', 'v_001413', 'v_001414', 'v_001415', 'v_001416', 'v_001417', 'v_001418', 'v_001419', 'v_001420', 'v_001421', 'v_001422', 'v_001423', 'v_001424', 'v_001425', 'v_001426', 'v_001427', 'v_001428', 'v_001429', 'v_001430', 'v_001431', 'v_001432', 'v_001433', 'v_001434', 'v_001435', 'v_001436', 'v_001437', 'v_001438', 'v_001439', 'v_001440', 'v_001441', 'v_001442', 'v_001443', 'v_001444', 'v_001445', 'v_001446', 'v_001447', 'v_001448', 'v_001449', 'v_001450', 'v_001451', 'v_001452', 'v_001453', 'v_001454', 'v_001455', 'v_001456', 'v_001457', 'v_001458', 'v_001459', 'v_001460', 'v_001461', 'v_001462', 'v_001463', 'v_001464', 'v_001465', 'v_001466', 'v_001467', 'v_001468', 'v_001469', 'v_001470', 'v_001471', 'v_001472', 'v_001473', 'v_001474', 'v_001475', 'v_001476', 'v_001477', 'v_001478', 'v_001479', 'v_001480', 'v_001481', 'v_001482', 'v_001483', 'v_001484', 'v_001485', 'v_001486', 'v_001487', 'v_001488', 'v_001489', 'v_001490', 'v_001491', 'v_001492', 'v_001493', 'v_001494', 'v_001495', 'v_001496', 'v_001497', 'v_001498', 'v_001499', 'v_001500', 'v_001501', 'v_001502', 'v_001503', 'v_001504', 'v_001505', 'v_001506', 'v_001507', 'v_001508', 'v_001549', 'v_001550', 'v_001551', 'v_001552', 'v_001553', 'v_001554', 'v_001555', 'v_001556', 'v_001557', 'v_001558', 'v_001559', 'v_001560', 'v_001561', 'v_001562', 'v_001563', 'v_001564', 'v_001565', 'v_001566', 'v_001567', 'v_001568', 'v_001569', 'v_001570', 'v_001571', 'v_001572', 'v_001573', 'v_001574', 'v_001575', 'v_001576', 'v_001577', 'v_001578', 'v_001579', 'v_001580', 'v_001581', 'v_001582', 'v_001583', 'v_001584', 'v_001585', 'v_001586', 'v_001587', 'v_001588', 'v_001589', 'v_001590', 'v_001591', 'v_001592', 'v_001593', 'v_001594', 'v_001595', 'v_001596', 'v_001597', 'v_001598', 'v_001599', 'v_001600', 'v_001601', 'v_001602', 'v_001603', 'v_001604', 'v_001605', 'v_001606', 'v_001607', 'v_001608', 'v_001609', 'v_001610', 'v_001611', 'v_001612', 'v_001613', 'v_001614', 'v_001615', 'v_001616', 'v_001617', 'v_001618', 'v_001619', 'v_001620', 'v_001621', 'v_001622', 'v_001623', 'v_001624', 'v_001625', 'v_001626', 'v_001627', 'v_001628', 'v_001629', 'v_001630', 'v_001631', 'v_001632', 'v_001633', 'v_001634', 'v_001635', 'v_001636', 'v_001637', 'v_001638', 'v_001639', 'v_001640', 'v_001641', 'v_001642', 'v_001643', 'v_001644', 'v_001645', 'v_001646', 'v_001647', 'v_001648', 'v_001649', 'v_001650', 'v_001651', 'v_001652', 'v_001653', 'v_001654', 'v_001655', 'v_001656', 'v_001657', 'v_001658', 'v_001697', 'v_001698', 'v_001699', 'v_001700', 'v_001701', 'v_001702', 'v_001703', 'v_001704', 'v_001705', 'v_001706', 'v_001707', 'v_001708', 'v_001709', 'v_001710', 'v_001711', 'v_001712', 'v_001713', 'v_001714', 'v_001715', 'v_001716', 'v_001717', 'v_001718', 'v_001719', 'v_001720', 'v_001721', 'v_001722', 'v_001723', 'v_001724', 'v_001725', 'v_001726', 'v_001727', 'v_001728', 'v_001729', 'v_001730', 'v_001731', 'v_001732', 'v_001733', 'v_001734', 'v_001735', 'v_001736', 'v_001737', 'v_001738', 'v_001739', 'v_001740', 'v_001741', 'v_001742', 'v_001743', 'v_001744', 'v_001745', 'v_001746', 'v_001747', 'v_001748', 'v_001749', 'v_001750', 'v_001751', 'v_001752', 'v_001753', 'v_001754', 'v_001755', 'v_001756', 'v_001757', 'v_001758', 'v_001759', 'v_001760', 'v_001761', 'v_001762', 'v_001763', 'v_001764', 'v_001765', 'v_001766', 'v_001767', 'v_001768', 'v_001769', 'v_001770', 'v_001771', 'v_001772', 'v_001773', 'v_001774', 'v_001775', 'v_001776', 'v_001777', 'v_001778', 'v_001779', 'v_001780', 'v_001781', 'v_001782', 'v_001783', 'v_001784', 'v_001785', 'v_001786', 'v_001787', 'v_001788', 'v_001789', 'v_001823', 'v_001824', 'v_001825', 'v_001826', 'v_001827', 'v_001828', 'v_001829', 'v_001830', 'v_001831', 'v_001832', 'v_001833', 'v_001834', 'v_001835', 'v_001836', 'v_001837', 'v_001838', 'v_001839', 'v_001840', 'v_001841', 'v_001842', 'v_001843', 'v_001844', 'v_001845', 'v_001846', 'v_001847', 'v_001848', 'v_001849', 'v_001850', 'v_001851', 'v_001852', 'v_001853', 'v_001854', 'v_001855', 'v_001856', 'v_001857', 'v_001858', 'v_001859', 'v_001860', 'v_001861', 'v_001862', 'v_001863', 'v_001864', 'v_001865', 'v_001866', 'v_001867', 'v_001868', 'v_001869', 'v_001870', 'v_001871', 'v_001872', 'v_001873', 'v_001874', 'v_001875', 'v_001876', 'v_001877', 'v_001878', 'v_001879', 'v_001880', 'v_001881', 'v_001882', 'v_001883', 'v_001884', 'v_001885', 'v_001886', 'v_001887', 'v_001888', 'v_001889', 'v_001890', 'v_001891', 'v_001892', 'v_001893', 'v_001894', 'v_001895', 'v_001896', 'v_001897', 'v_001898', 'v_001929', 'v_001930', 'v_001931', 'v_001932', 'v_001933', 'v_001934', 'v_001935', 'v_001936', 'v_001937', 'v_001938', 'v_001939', 'v_001940', 'v_001941', 'v_001942', 'v_001943', 'v_001944', 'v_001945', 'v_001946', 'v_001947', 'v_001948', 'v_001949', 'v_001950', 'v_001951', 'v_001952', 'v_001953', 'v_001954', 'v_001955', 'v_001956', 'v_001957', 'v_001958', 'v_001959', 'v_001960', 'v_001961', 'v_001962', 'v_001963', 'v_001964', 'v_001965', 'v_001966', 'v_001967', 'v_001968', 'v_001969', 'v_001970', 'v_001971', 'v_001972', 'v_001973', 'v_001974', 'v_001975', 'v_001976', 'v_001977', 'v_001978', 'v_001979', 'v_001980', 'v_001981', 'v_001982', 'v_001983', 'v_001984', 'v_001985', 'v_001986', 'v_001987', 'v_001988', 'v_001989', 'v_001990', 'v_001991', 'v_001992', 'v_001993', 'v_001994', 'v_001995', 'v_001996', 'v_001997', 'v_001998', 'v_001999', 'v_002000', 'v_002001', 'v_002002', 'v_002003', 'v_002004', 'v_002005', 'v_002006', 'v_002007', 'v_002008', 'v_002009', 'v_002010', 'v_002054', 'v_002055', 'v_002056', 'v_002057', 'v_002058', 'v_002059', 'v_002060', 'v_002061', 'v_002062', 'v_002063', 'v_002064', 'v_002065', 'v_002066', 'v_002067', 'v_002068', 'v_002069', 'v_002070', 'v_002071', 'v_002072', 'v_002073', 'v_002074', 'v_002075', 'v_002076', 'v_002077', 'v_002078', 'v_002079', 'v_002080', 'v_002081', 'v_002082', 'v_002083', 'v_002084', 'v_002085', 'v_002086', 'v_002087', 'v_002088', 'v_002089', 'v_002090', 'v_002091', 'v_002092', 'v_002093', 'v_002094', 'v_002095', 'v_002096', 'v_002097', 'v_002098', 'v_002099', 'v_002100', 'v_002101', 'v_002102', 'v_002103', 'v_002104', 'v_002105', 'v_002106', 'v_002107', 'v_002108', 'v_002109', 'v_002110', 'v_002111', 'v_002112', 'v_002113', 'v_002114', 'v_002115', 'v_002116', 'v_002117', 'v_002118', 'v_002119', 'v_002120', 'v_002121', 'v_002122', 'v_002123', 'v_002124', 'v_002125', 'v_002126', 'v_002127', 'v_002128', 'v_002129', 'v_002130', 'v_002131', 'v_002132', 'v_002133', 'v_002134', 'v_002135', 'v_002136', 'v_002137', 'v_002138', 'v_002139', 'v_002140', 'v_002141', 'v_002142', 'v_002143', 'v_002144', 'v_002145', 'v_002146', 'v_002147', 'v_002148', 'v_002149', 'v_002150', 'v_002151', 'v_002152', 'v_002153', 'v_002154', 'v_002155', 'v_002156', 'v_002157', 'v_002158', 'v_002159', 'v_002160', 'v_002161', 'v_002162', 'v_002163', 'v_002164', 'v_002165', 'v_002215', 'v_002216', 'v_002217', 'v_002218', 'v_002219', 'v_002220', 'v_002221', 'v_002222', 'v_002223', 'v_002224', 'v_002225', 'v_002226', 'v_002227', 'v_002228', 'v_002229', 'v_002230', 'v_002231', 'v_002232', 'v_002233', 'v_002234', 'v_002235', 'v_002236', 'v_002237', 'v_002238', 'v_002239', 'v_002240', 'v_002241', 'v_002242', 'v_002243', 'v_002244', 'v_002245', 'v_002246', 'v_002247', 'v_002248', 'v_002249', 'v_002250', 'v_002251', 'v_002252', 'v_002253', 'v_002254', 'v_002255', 'v_002256', 'v_002257', 'v_002258', 'v_002259', 'v_002260', 'v_002261', 'v_002262', 'v_002263', 'v_002264', 'v_002265', 'v_002266', 'v_002267', 'v_002268', 'v_002269', 'v_002270', 'v_002271', 'v_002272', 'v_002273', 'v_002274', 'v_002275', 'v_002276', 'v_002277', 'v_002278', 'v_002279', 'v_002280', 'v_002281', 'v_002282', 'v_002283', 'v_002284', 'v_002285', 'v_002286', 'v_002287', 'v_002288', 'v_002289', 'v_002290', 'v_002291', 'v_002292', 'v_002293', 'v_002294', 'v_002295', 'v_002296', 'v_002297', 'v_002298', 'v_002299', 'v_002300', 'v_002301', 'v_002302', 'v_002303', 'v_002304', 'v_002305', 'v_002306', 'v_002307', 'v_002308', 'v_002309', 'v_002310', 'v_002311', 'v_002312', 'v_002313', 'v_002314', 'v_002315', 'v_002316', 'v_002317', 'v_002318', 'v_002319', 'v_002320', 'v_002321', 'v_002322', 'v_002323', 'v_002324', 'v_002325', 'v_002326', 'v_002327', 'v_002328', 'v_002366', 'v_002367', 'v_002368', 'v_002369', 'v_002370', 'v_002371', 'v_002372', 'v_002373', 'v_002374', 'v_002375', 'v_002376', 'v_002377', 'v_002378', 'v_002379', 'v_002380', 'v_002381', 'v_002382', 'v_002383', 'v_002384', 'v_002385', 'v_002386', 'v_002387', 'v_002388', 'v_002389', 'v_002390', 'v_002391', 'v_002392', 'v_002393', 'v_002394', 'v_002395', 'v_002396', 'v_002397', 'v_002398', 'v_002399', 'v_002400', 'v_002401', 'v_002402', 'v_002403', 'v_002404', 'v_002405', 'v_002406', 'v_002407', 'v_002408', 'v_002409', 'v_002410', 'v_002411', 'v_002412', 'v_002413', 'v_002414', 'v_002415', 'v_002416', 'v_002417', 'v_002418', 'v_002419', 'v_002420', 'v_002421', 'v_002422', 'v_002423', 'v_002424', 'v_002425', 'v_002426', 'v_002427', 'v_002428', 'v_002429', 'v_002430', 'v_002431', 'v_002432', 'v_002433', 'v_002434', 'v_002435', 'v_002436', 'v_002437', 'v_002438', 'v_002439', 'v_002440', 'v_002441', 'v_002442', 'v_002443', 'v_002444', 'v_002445', 'v_002446', 'v_002447', 'v_002448', 'v_002449', 'v_002450', 'v_002451', 'v_002452', 'v_002453', 'v_002454', 'v_002455', 'v_002456', 'v_002457', 'v_002458', 'v_002459', 'v_002460', 'v_002461', 'v_002462', 'v_002491', 'v_002492', 'v_002493', 'v_002494', 'v_002495', 'v_002496', 'v_002497', 'v_002498', 'v_002499', 'v_002500', 'v_002501', 'v_002502', 'v_002503', 'v_002504', 'v_002505', 'v_002506', 'v_002507', 'v_002508', 'v_002509', 'v_002510', 'v_002511', 'v_002512', 'v_002513', 'v_002514', 'v_002515', 'v_002516', 'v_002517', 'v_002518', 'v_002519', 'v_002520', 'v_002521', 'v_002522', 'v_002523', 'v_002524', 'v_002525', 'v_002526', 'v_002527', 'v_002528', 'v_002529', 'v_002530', 'v_002531', 'v_002532', 'v_002533', 'v_002534', 'v_002535', 'v_002536', 'v_002537', 'v_002538', 'v_002539', 'v_002540', 'v_002541', 'v_002542', 'v_002543', 'v_002544', 'v_002545', 'v_002546', 'v_002547', 'v_002548', 'v_002549', 'v_002550', 'v_002551', 'v_002552', 'v_002553', 'v_002554', 'v_002555', 'v_002556', 'v_002557', 'v_002558', 'v_002559', 'v_002560', 'v_002561', 'v_002562', 'v_002563', 'v_002600', 'v_002601', 'v_002602', 'v_002603', 'v_002604', 'v_002605', 'v_002606', 'v_002607', 'v_002608', 'v_002609', 'v_002610', 'v_002611', 'v_002612', 'v_002613', 'v_002614', 'v_002615', 'v_002616', 'v_002617', 'v_002618', 'v_002619', 'v_002620', 'v_002621', 'v_002622', 'v_002623', 'v_002624', 'v_002625', 'v_002626', 'v_002627', 'v_002628', 'v_002629', 'v_002630', 'v_002631', 'v_002632', 'v_002633', 'v_002634', 'v_002635', 'v_002636', 'v_002637', 'v_002638', 'v_002639', 'v_002640', 'v_002641', 'v_002642', 'v_002643', 'v_002644', 'v_002645', 'v_002646', 'v_002647', 'v_002648', 'v_002649', 'v_002650', 'v_002651', 'v_002652', 'v_002653', 'v_002654', 'v_002655', 'v_002656', 'v_002657', 'v_002658', 'v_002659', 'v_002660', 'v_002661', 'v_002662', 'v_002663', 'v_002664', 'v_002665', 'v_002666', 'v_002667', 'v_002668', 'v_002669', 'v_002670', 'v_002671', 'v_002672', 'v_002673', 'v_002674', 'v_002675', 'v_002676', 'v_002677', 'v_002678', 'v_002679', 'v_002680', 'v_002681', 'v_002682', 'v_002683', 'v_002684', 'v_002685', 'v_002686', 'v_002687', 'v_002688', 'v_002689', 'v_002690', 'v_002691', 'v_002692', 'v_002693', 'v_002694', 'v_002728', 'v_002729', 'v_002730', 'v_002731', 'v_002732', 'v_002733', 'v_002734', 'v_002735', 'v_002736', 'v_002737', 'v_002738', 'v_002739', 'v_002740', 'v_002741', 'v_002742', 'v_002743', 'v_002744', 'v_002745', 'v_002746', 'v_002747', 'v_002748', 'v_002749', 'v_002750', 'v_002751', 'v_002752', 'v_002753', 'v_002754', 'v_002755', 'v_002756', 'v_002757', 'v_002758', 'v_002759', 'v_002760', 'v_002761', 'v_002762', 'v_002763', 'v_002764', 'v_002765', 'v_002766', 'v_002767', 'v_002768', 'v_002769', 'v_002770', 'v_002771', 'v_002772', 'v_002773', 'v_002774', 'v_002775', 'v_002776', 'v_002777', 'v_002778', 'v_002779', 'v_002780', 'v_002781', 'v_002782', 'v_002783', 'v_002784', 'v_002785', 'v_002786', 'v_002787', 'v_002788', 'v_002789', 'v_002790', 'v_002791', 'v_002792', 'v_002793', 'v_002794', 'v_002795', 'v_002796', 'v_002797', 'v_002798', 'v_002799', 'v_002800', 'v_002801', 'v_002802', 'v_002803', 'v_002804', 'v_002805', 'v_002806', 'v_002846', 'v_002847', 'v_002848', 'v_002849', 'v_002850', 'v_002851', 'v_002852', 'v_002853', 'v_002854', 'v_002855', 'v_002856', 'v_002857', 'v_002858', 'v_002859', 'v_002860', 'v_002861', 'v_002862', 'v_002863', 'v_002864', 'v_002865', 'v_002866', 'v_002867', 'v_002868', 'v_002869', 'v_002870', 'v_002871', 'v_002872', 'v_002873', 'v_002874', 'v_002875', 'v_002876', 'v_002877', 'v_002878', 'v_002879', 'v_002880', 'v_002881', 'v_002882', 'v_002883', 'v_002884', 'v_002885', 'v_002886', 'v_002887', 'v_002888', 'v_002889', 'v_002890', 'v_002891', 'v_002892', 'v_002893', 'v_002894', 'v_002895', 'v_002896', 'v_002897', 'v_002898', 'v_002899', 'v_002900', 'v_002901', 'v_002902', 'v_002903', 'v_002904', 'v_002905', 'v_002906', 'v_002907', 'v_002908', 'v_002909', 'v_002910', 'v_002911', 'v_002912', 'v_002913', 'v_002914', 'v_002915', 'v_002916', 'v_002917', 'v_002918', 'v_002919', 'v_002920', 'v_002921', 'v_002922', 'v_002923', 'v_002924', 'v_002925', 'v_002926', 'v_002927', 'v_002928', 'v_002929', 'v_002930', 'v_002931', 'v_002932', 'v_002933', 'v_002934', 'v_002935', 'v_002936', 'v_002937', 'v_002938', 'v_002939', 'v_002940', 'v_002941', 'v_002942', 'v_002943', 'v_002944', 'v_002981', 'v_002982', 'v_002983', 'v_002984', 'v_002985', 'v_002986', 'v_002987', 'v_002988', 'v_002989', 'v_002990', 'v_002991', 'v_002992', 'v_002993', 'v_002994', 'v_002995', 'v_002996', 'v_002997', 'v_002998', 'v_002999', 'v_003000', 'v_003001', 'v_003002', 'v_003003', 'v_003004', 'v_003005', 'v_003006', 'v_003007', 'v_003008', 'v_003009', 'v_003010', 'v_003011', 'v_003012', 'v_003013', 'v_003014', 'v_003015', 'v_003016', 'v_003017', 'v_003018', 'v_003019', 'v_003020', 'v_003021', 'v_003022', 'v_003023', 'v_003024', 'v_003025', 'v_003026', 'v_003027', 'v_003028', 'v_003029', 'v_003030', 'v_003031', 'v_003032', 'v_003033', 'v_003034', 'v_003035', 'v_003036', 'v_003037', 'v_003038', 'v_003039', 'v_003040', 'v_003041', 'v_003042', 'v_003043', 'v_003044', 'v_003045', 'v_003046', 'v_003047', 'v_003048', 'v_003049', 'v_003050', 'v_003051', 'v_003052', 'v_003053', 'v_003054', 'v_003055', 'v_003056', 'v_003057', 'v_003058', 'v_003059', 'v_003060', 'v_003061', 'v_003062', 'v_003063', 'v_003064', 'v_003065', 'v_003066', 'v_003067', 'v_003068', 'v_003069', 'v_003070', 'v_003071', 'v_003072', 'v_003073', 'v_003074', 'v_003075', 'v_003076', 'v_003077', 'v_003078', 'v_003079', 'v_003080', 'v_003081', 'v_003082', 'v_003083', 'v_003133', 'v_003134', 'v_003135', 'v_003136', 'v_003137', 'v_003138', 'v_003139', 'v_003140', 'v_003141', 'v_003142', 'v_003143', 'v_003144', 'v_003145', 'v_003146', 'v_003147', 'v_003148', 'v_003149', 'v_003150', 'v_003151', 'v_003152', 'v_003153', 'v_003154', 'v_003155', 'v_003156', 'v_003157', 'v_003158', 'v_003159', 'v_003160', 'v_003161', 'v_003162', 'v_003163', 'v_003164', 'v_003165', 'v_003166', 'v_003167', 'v_003168', 'v_003169', 'v_003170', 'v_003171', 'v_003172', 'v_003173', 'v_003174', 'v_003175', 'v_003176', 'v_003177', 'v_003178', 'v_003179', 'v_003180', 'v_003181', 'v_003182', 'v_003183', 'v_003184', 'v_003185', 'v_003186', 'v_003187', 'v_003188', 'v_003189', 'v_003190', 'v_003191', 'v_003192', 'v_003193', 'v_003194', 'v_003195', 'v_003196', 'v_003197', 'v_003198', 'v_003199', 'v_003200', 'v_003201', 'v_003202', 'v_003203', 'v_003204', 'v_003205', 'v_003206', 'v_003207', 'v_003208', 'v_003209', 'v_003210', 'v_003211', 'v_003212', 'v_003213', 'v_003214', 'v_003215', 'v_003216', 'v_003217', 'v_003218', 'v_003219', 'v_003220', 'v_003221', 'v_003222', 'v_003223', 'v_003224', 'v_003225', 'v_003226', 'v_003227', 'v_003228', 'v_003229', 'v_003230', 'v_003231', 'v_003232', 'v_003233', 'v_003234', 'v_003235', 'v_003236', 'v_003237', 'v_003238', 'v_003239', 'v_003240', 'v_003241', 'v_003242', 'v_003243', 'v_003244', 'v_003245', 'v_003246', 'v_003247', 'v_003248', 'v_003249', 'v_003250', 'v_003284', 'v_003285', 'v_003286', 'v_003287', 'v_003288', 'v_003289', 'v_003290', 'v_003291', 'v_003292', 'v_003293', 'v_003294', 'v_003295', 'v_003296', 'v_003297', 'v_003298', 'v_003299', 'v_003300', 'v_003301', 'v_003302', 'v_003303', 'v_003304', 'v_003305', 'v_003306', 'v_003307', 'v_003308', 'v_003309', 'v_003310', 'v_003311', 'v_003312', 'v_003313', 'v_003314', 'v_003315', 'v_003316', 'v_003317', 'v_003318', 'v_003319', 'v_003320', 'v_003321', 'v_003322', 'v_003323', 'v_003324', 'v_003325', 'v_003326', 'v_003327', 'v_003328', 'v_003329', 'v_003330', 'v_003331', 'v_003332', 'v_003333', 'v_003334', 'v_003335', 'v_003336', 'v_003337', 'v_003338', 'v_003339', 'v_003340', 'v_003341', 'v_003342', 'v_003343', 'v_003344', 'v_003345', 'v_003346', 'v_003347', 'v_003348', 'v_003349', 'v_003350', 'v_003351', 'v_003352', 'v_003353', 'v_003354', 'v_003355', 'v_003356', 'v_003357', 'v_003358', 'v_003359', 'v_003360']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kp_3q8SoEctD","colab_type":"code","outputId":"8e297047-0d14-4466-ac8a-f83e2f05a314","executionInfo":{"status":"ok","timestamp":1574364238001,"user_tz":300,"elapsed":1442,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from torch.utils.data import Dataset,DataLoader\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.enabled = True\n","\n","import gc\n","\n","class UCF101Dataset(Dataset):\n","    \n","    def __init__(self, data_list, labels_list, root_dir, vgg16):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        #self.dataframe = dataframe\n","        self.data_list = data_list\n","        self.labels_list = labels_list\n","        self.root_dir = root_dir\n","        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        self.prep = torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), self.normalize ])\n","        #self.transform = transform\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        #self.vgg16 = vgg16\n","        print(self.device)\n","        self.vgg16 = vgg16.to(self.device)\n","        \n","        del self.vgg16.classifier[1:]\n","        print(self.vgg16.classifier)\n","        self.length = len(self.data_list)*25\n","        #print(\"init\", self.length)\n","\n","    def __len__(self):\n","        #print(\"len\")\n","        return (self.length)\n","\n","    def __getitem__(self, idx):\n","        #if torch.is_tensor(idx):\n","        #    idx = idx.tolist()\n","        #print(\"folder\")\n","        folder_name = os.path.join(self.root_dir,\n","                                self.data_list[idx//25])\n","        \n","        files = os.listdir(folder_name)\n","        #print(files)\n","        #for i in range(len(files)):\n","          \n","        file_name = os.path.join(folder_name, files[idx%25])\n","          \n","        #img_name = img_name + '/*.jpg' \n","        #image = cv2.imread(file_name)\n","        label = self.labels_list[idx//25]\n","\n","        #if idx % 25 == 0:\n","          #print(folder_name, label)\n","        #gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        #print(file_name)\n","        img = cv2.imread(file_name)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = self.prep(img)\n","        #print(img.size()[0])\n","        img_c = img[ :, (img.shape[1]-224)//2:(img.shape[1]+224)//2, (img.shape[2]-224)//2:(img.shape[2]+224)//2]\n","        img_tl = img[:, :224, :224] \n","        img_tr = img[:, :224, (img.shape[2]-224):]\n","        img_bl = img[:, (img.shape[1]-224):, :224] \n","        img_br = img[:, (img.shape[1]-224):, (img.shape[2]-224):] \n","        #print(len(img_tr[2]), len(img_bl[1]), len(img_br[0]))\n","        #print(img_c.size(), img_tl.size(), img_tr.size(), img_bl.size(), img_br.size())\n","        with torch.no_grad():\n","          inputs = torch.stack([img_c, img_tl, img_tr, img_bl, img_br])\n","          #print(len(inputs))\n","          inputs = inputs.to(self.device)\n","          #img_feat = self.vgg16(inputs)\n","          vgg16_local = self.vgg16\n","          img_feat1 = vgg16_local(inputs)\n","          img_feat1 = img_feat1.cpu()\n","          #print(inputs.is_cuda, img_feat1.is_cuda, img_c.is_cuda)#, img_feat1.is_cuda)\n","          del inputs, vgg16_local\n","          torch.cuda.empty_cache()\n","          #gc.collect()\n","          #img_feat = self.vgg16(img_c)\n","          #print(len(img_feat), len(img_feat[0]),len(img_feat[1]),len(img_feat[2]))\n","          img_feat1 = img_feat1.mean(dim=0)\n","          #print(len(img_feat1))\n","          #torch.cuda.empty_cache()\n","          \n","\n","        return (img_feat1, label)\n","        #landmarks = np.array([landmarks])\n","        #landmarks = landmarks.astype('float').reshape(-1, 2)\n","        #sample = {'image': image, 'landmarks': landmarks}\n","\n","        \n","\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IA1x7F0nt1vQ","colab_type":"code","outputId":"bc75fbf9-1b0f-4385-fe89-4435c32f1b6f","executionInfo":{"status":"ok","timestamp":1574479029515,"user_tz":300,"elapsed":606,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from torch.utils.data import Dataset,DataLoader\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.enabled = True\n","\n","import gc\n","\n","class UCF101Dataset(Dataset):\n","    \n","    def __init__(self, data_list, labels_list, root_dir, vgg16):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        #self.dataframe = dataframe\n","        self.data_list = data_list\n","        self.labels_list = labels_list\n","        self.root_dir = root_dir\n","        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        self.prep = torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), self.normalize ])\n","        #self.transform = transform\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        #self.vgg16 = vgg16\n","        print(self.device)\n","        self.vgg16 = vgg16.to(self.device)\n","        \n","        del self.vgg16.classifier[1:]\n","        print(self.vgg16.classifier)\n","        self.length = len(self.data_list)\n","        #print(\"init\", self.length)\n","\n","    def __len__(self):\n","        #print(\"len\")\n","        return (self.length)\n","\n","    def __getitem__(self, idx):\n","        #if torch.is_tensor(idx):\n","        #    idx = idx.tolist()\n","        #print(\"folder\")\n","        folder_name = os.path.join(self.root_dir,\n","                                self.data_list[idx])\n","        label = self.labels_list[idx]\n","        \n","        files = os.listdir(folder_name)\n","        #print(files)\n","        #for i in range(len(files)):\n","        i = 0\n","        feats25 = torch.empty(0)\n","        while i < 25:\n","          \n","          file_name = os.path.join(folder_name, files[i])\n","            \n","          #img_name = img_name + '/*.jpg' \n","          #image = cv2.imread(file_name)\n","          \n","\n","          #if idx % 25 == 0:\n","            #print(folder_name, label)\n","          gc.collect()\n","          torch.cuda.empty_cache()\n","\n","          #print(file_name)\n","          img = cv2.imread(file_name)\n","          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","          img = self.prep(img)\n","          #print(img.size()[0])\n","          img_c = img[ :, (img.shape[1]-224)//2:(img.shape[1]+224)//2, (img.shape[2]-224)//2:(img.shape[2]+224)//2]\n","          img_tl = img[:, :224, :224] \n","          img_tr = img[:, :224, (img.shape[2]-224):]\n","          img_bl = img[:, (img.shape[1]-224):, :224] \n","          img_br = img[:, (img.shape[1]-224):, (img.shape[2]-224):] \n","          #print(len(img_tr[2]), len(img_bl[1]), len(img_br[0]))\n","          #print(img_c.size(), img_tl.size(), img_tr.size(), img_bl.size(), img_br.size())\n","          with torch.no_grad():\n","            inputs = torch.stack([img_c, img_tl, img_tr, img_bl, img_br])\n","            #print(len(inputs))\n","            inputs = inputs.to(self.device)\n","            #img_feat = self.vgg16(inputs)\n","            vgg16_local = self.vgg16\n","            img_feat1 = vgg16_local(inputs)\n","            img_feat1 = img_feat1.cpu()\n","            #print(inputs.is_cuda, img_feat1.is_cuda, img_c.is_cuda)#, img_feat1.is_cuda)\n","            del inputs, vgg16_local\n","            torch.cuda.empty_cache()\n","            #gc.collect()\n","            #img_feat = self.vgg16(img_c)\n","            #print(len(img_feat), len(img_feat[0]),len(img_feat[1]),len(img_feat[2]))\n","            img_feat1 = img_feat1.mean(dim=0)\n","            if i == 0:\n","              feats25 = img_feat1\n","            else:  \n","              feats25 = torch.cat((feats25, img_feat1), 0)\n","            del img_feat1  \n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            #print(len(img_feat1))\n","            #torch.cuda.empty_cache()\n","\n","            i += 1\n","          \n","        #print(len(feats25), label)\n","        return (feats25, label)\n","        #landmarks = np.array([landmarks])\n","        #landmarks = landmarks.astype('float').reshape(-1, 2)\n","        #sample = {'image': image, 'landmarks': landmarks}\n","\n","        \n","\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kdAVT-_Y-5iD","colab_type":"code","outputId":"5463e3cf-2ca8-4ffc-c900-580feee14809","executionInfo":{"status":"ok","timestamp":1574479033981,"user_tz":300,"elapsed":363,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["print(len(train25_folders), len(train25_labels))\n","train25_folders1 = train25_folders[:500]\n","train25_folders2 = train25_folders[500:1000]\n","train25_folders3 = train25_folders[1000:1500]\n","train25_folders4 = train25_folders[1500:2000]\n","train25_folders5 = train25_folders[2000:]\n","print(len(train25_folders1), len(train25_folders2), len(train25_folders3), len(train25_folders4), len(train25_folders5))\n","\n","train25_labels1 = train25_labels[:500]\n","train25_labels2 = train25_labels[500:1000]\n","train25_labels3 = train25_labels[1000:1500]\n","train25_labels4 = train25_labels[1500:2000]\n","train25_labels5 = train25_labels[2000:]\n","print(len(train25_labels1), len(train25_labels2), len(train25_labels3), len(train25_labels4), len(train25_labels5))\n","\n","print(train25_folders1) \n","print(train25_labels1)\n","print(train25_folders2)\n","print(train25_labels2)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2409 2409\n","500 500 500 500 409\n","500 500 500 500 409\n","['v_000045', 'v_000046', 'v_000047', 'v_000048', 'v_000049', 'v_000050', 'v_000051', 'v_000052', 'v_000053', 'v_000054', 'v_000055', 'v_000056', 'v_000057', 'v_000058', 'v_000059', 'v_000060', 'v_000061', 'v_000062', 'v_000063', 'v_000064', 'v_000065', 'v_000066', 'v_000067', 'v_000068', 'v_000069', 'v_000070', 'v_000071', 'v_000072', 'v_000073', 'v_000074', 'v_000075', 'v_000076', 'v_000077', 'v_000078', 'v_000079', 'v_000080', 'v_000081', 'v_000082', 'v_000083', 'v_000084', 'v_000085', 'v_000086', 'v_000087', 'v_000088', 'v_000089', 'v_000090', 'v_000091', 'v_000092', 'v_000093', 'v_000094', 'v_000095', 'v_000096', 'v_000097', 'v_000098', 'v_000099', 'v_000100', 'v_000101', 'v_000102', 'v_000103', 'v_000104', 'v_000105', 'v_000106', 'v_000107', 'v_000108', 'v_000109', 'v_000110', 'v_000111', 'v_000112', 'v_000113', 'v_000114', 'v_000115', 'v_000116', 'v_000117', 'v_000118', 'v_000119', 'v_000120', 'v_000121', 'v_000122', 'v_000123', 'v_000124', 'v_000125', 'v_000126', 'v_000127', 'v_000128', 'v_000129', 'v_000130', 'v_000131', 'v_000132', 'v_000133', 'v_000134', 'v_000135', 'v_000136', 'v_000137', 'v_000138', 'v_000139', 'v_000140', 'v_000141', 'v_000142', 'v_000143', 'v_000144', 'v_000145', 'v_000178', 'v_000179', 'v_000180', 'v_000181', 'v_000182', 'v_000183', 'v_000184', 'v_000185', 'v_000186', 'v_000187', 'v_000188', 'v_000189', 'v_000190', 'v_000191', 'v_000192', 'v_000193', 'v_000194', 'v_000195', 'v_000196', 'v_000197', 'v_000198', 'v_000199', 'v_000200', 'v_000201', 'v_000202', 'v_000203', 'v_000204', 'v_000205', 'v_000206', 'v_000207', 'v_000208', 'v_000209', 'v_000210', 'v_000211', 'v_000212', 'v_000213', 'v_000214', 'v_000215', 'v_000216', 'v_000217', 'v_000218', 'v_000219', 'v_000220', 'v_000221', 'v_000222', 'v_000223', 'v_000224', 'v_000225', 'v_000226', 'v_000227', 'v_000228', 'v_000229', 'v_000230', 'v_000231', 'v_000232', 'v_000233', 'v_000234', 'v_000235', 'v_000236', 'v_000237', 'v_000238', 'v_000239', 'v_000240', 'v_000241', 'v_000242', 'v_000243', 'v_000244', 'v_000245', 'v_000246', 'v_000247', 'v_000248', 'v_000249', 'v_000250', 'v_000251', 'v_000252', 'v_000253', 'v_000254', 'v_000255', 'v_000256', 'v_000257', 'v_000258', 'v_000259', 'v_000301', 'v_000302', 'v_000303', 'v_000304', 'v_000305', 'v_000306', 'v_000307', 'v_000308', 'v_000309', 'v_000310', 'v_000311', 'v_000312', 'v_000313', 'v_000314', 'v_000315', 'v_000316', 'v_000317', 'v_000318', 'v_000319', 'v_000320', 'v_000321', 'v_000322', 'v_000323', 'v_000324', 'v_000325', 'v_000326', 'v_000327', 'v_000328', 'v_000329', 'v_000330', 'v_000331', 'v_000332', 'v_000333', 'v_000334', 'v_000335', 'v_000336', 'v_000337', 'v_000338', 'v_000339', 'v_000340', 'v_000341', 'v_000342', 'v_000343', 'v_000344', 'v_000345', 'v_000346', 'v_000347', 'v_000348', 'v_000349', 'v_000350', 'v_000351', 'v_000352', 'v_000353', 'v_000354', 'v_000355', 'v_000356', 'v_000357', 'v_000358', 'v_000359', 'v_000360', 'v_000361', 'v_000362', 'v_000363', 'v_000364', 'v_000365', 'v_000366', 'v_000367', 'v_000368', 'v_000369', 'v_000370', 'v_000371', 'v_000372', 'v_000373', 'v_000374', 'v_000375', 'v_000376', 'v_000377', 'v_000378', 'v_000379', 'v_000380', 'v_000381', 'v_000382', 'v_000383', 'v_000384', 'v_000385', 'v_000386', 'v_000387', 'v_000388', 'v_000389', 'v_000390', 'v_000391', 'v_000392', 'v_000393', 'v_000394', 'v_000395', 'v_000396', 'v_000397', 'v_000398', 'v_000399', 'v_000400', 'v_000401', 'v_000402', 'v_000403', 'v_000404', 'v_000440', 'v_000441', 'v_000442', 'v_000443', 'v_000444', 'v_000445', 'v_000446', 'v_000447', 'v_000448', 'v_000449', 'v_000450', 'v_000451', 'v_000452', 'v_000453', 'v_000454', 'v_000455', 'v_000456', 'v_000457', 'v_000458', 'v_000459', 'v_000460', 'v_000461', 'v_000462', 'v_000463', 'v_000464', 'v_000465', 'v_000466', 'v_000467', 'v_000468', 'v_000469', 'v_000470', 'v_000471', 'v_000472', 'v_000473', 'v_000474', 'v_000475', 'v_000476', 'v_000477', 'v_000478', 'v_000479', 'v_000480', 'v_000481', 'v_000482', 'v_000483', 'v_000484', 'v_000485', 'v_000486', 'v_000487', 'v_000488', 'v_000489', 'v_000490', 'v_000491', 'v_000492', 'v_000493', 'v_000494', 'v_000495', 'v_000496', 'v_000497', 'v_000498', 'v_000499', 'v_000500', 'v_000501', 'v_000502', 'v_000503', 'v_000504', 'v_000505', 'v_000506', 'v_000507', 'v_000508', 'v_000509', 'v_000510', 'v_000511', 'v_000512', 'v_000513', 'v_000514', 'v_000515', 'v_000516', 'v_000517', 'v_000518', 'v_000519', 'v_000520', 'v_000521', 'v_000522', 'v_000523', 'v_000524', 'v_000525', 'v_000526', 'v_000527', 'v_000528', 'v_000529', 'v_000530', 'v_000531', 'v_000532', 'v_000533', 'v_000534', 'v_000535', 'v_000536', 'v_000568', 'v_000569', 'v_000570', 'v_000571', 'v_000572', 'v_000573', 'v_000574', 'v_000575', 'v_000576', 'v_000577', 'v_000578', 'v_000579', 'v_000580', 'v_000581', 'v_000582', 'v_000583', 'v_000584', 'v_000585', 'v_000586', 'v_000587', 'v_000588', 'v_000589', 'v_000590', 'v_000591', 'v_000592', 'v_000593', 'v_000594', 'v_000595', 'v_000596', 'v_000597', 'v_000598', 'v_000599', 'v_000600', 'v_000601', 'v_000602', 'v_000603', 'v_000604', 'v_000605', 'v_000606', 'v_000607', 'v_000608', 'v_000609', 'v_000610', 'v_000611', 'v_000612', 'v_000613', 'v_000614', 'v_000615', 'v_000616', 'v_000617', 'v_000618', 'v_000619', 'v_000620', 'v_000621', 'v_000622', 'v_000623', 'v_000624', 'v_000625', 'v_000626', 'v_000627', 'v_000628', 'v_000629', 'v_000630', 'v_000631', 'v_000632', 'v_000633', 'v_000634', 'v_000635', 'v_000636', 'v_000637', 'v_000638', 'v_000639', 'v_000640', 'v_000641', 'v_000642', 'v_000643', 'v_000644', 'v_000688', 'v_000689', 'v_000690', 'v_000691', 'v_000692', 'v_000693', 'v_000694', 'v_000695', 'v_000696', 'v_000697', 'v_000698', 'v_000699', 'v_000700', 'v_000701', 'v_000702', 'v_000703', 'v_000704', 'v_000705', 'v_000706', 'v_000707', 'v_000708', 'v_000709', 'v_000710', 'v_000711', 'v_000712', 'v_000713', 'v_000714', 'v_000715', 'v_000716', 'v_000717', 'v_000718', 'v_000719', 'v_000720', 'v_000721', 'v_000722', 'v_000723', 'v_000724', 'v_000725', 'v_000726']\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n","['v_000727', 'v_000728', 'v_000729', 'v_000730', 'v_000731', 'v_000732', 'v_000733', 'v_000734', 'v_000735', 'v_000736', 'v_000737', 'v_000738', 'v_000739', 'v_000740', 'v_000741', 'v_000742', 'v_000743', 'v_000744', 'v_000745', 'v_000746', 'v_000747', 'v_000748', 'v_000749', 'v_000750', 'v_000751', 'v_000752', 'v_000753', 'v_000754', 'v_000755', 'v_000756', 'v_000757', 'v_000758', 'v_000759', 'v_000760', 'v_000761', 'v_000762', 'v_000763', 'v_000764', 'v_000765', 'v_000766', 'v_000767', 'v_000768', 'v_000769', 'v_000770', 'v_000771', 'v_000772', 'v_000773', 'v_000774', 'v_000775', 'v_000776', 'v_000777', 'v_000778', 'v_000779', 'v_000780', 'v_000781', 'v_000782', 'v_000783', 'v_000784', 'v_000785', 'v_000786', 'v_000787', 'v_000788', 'v_000789', 'v_000790', 'v_000791', 'v_000792', 'v_000793', 'v_000794', 'v_000795', 'v_000796', 'v_000797', 'v_000798', 'v_000799', 'v_000843', 'v_000844', 'v_000845', 'v_000846', 'v_000847', 'v_000848', 'v_000849', 'v_000850', 'v_000851', 'v_000852', 'v_000853', 'v_000854', 'v_000855', 'v_000856', 'v_000857', 'v_000858', 'v_000859', 'v_000860', 'v_000861', 'v_000862', 'v_000863', 'v_000864', 'v_000865', 'v_000866', 'v_000867', 'v_000868', 'v_000869', 'v_000870', 'v_000871', 'v_000872', 'v_000873', 'v_000874', 'v_000875', 'v_000876', 'v_000877', 'v_000878', 'v_000879', 'v_000880', 'v_000881', 'v_000882', 'v_000883', 'v_000884', 'v_000885', 'v_000886', 'v_000887', 'v_000888', 'v_000889', 'v_000890', 'v_000891', 'v_000892', 'v_000893', 'v_000894', 'v_000895', 'v_000896', 'v_000897', 'v_000898', 'v_000899', 'v_000900', 'v_000901', 'v_000902', 'v_000903', 'v_000904', 'v_000905', 'v_000906', 'v_000907', 'v_000908', 'v_000909', 'v_000910', 'v_000911', 'v_000912', 'v_000913', 'v_000914', 'v_000915', 'v_000916', 'v_000917', 'v_000918', 'v_000919', 'v_000920', 'v_000921', 'v_000922', 'v_000923', 'v_000924', 'v_000925', 'v_000926', 'v_000927', 'v_000928', 'v_000929', 'v_000930', 'v_000931', 'v_000932', 'v_000933', 'v_000934', 'v_000935', 'v_000936', 'v_000937', 'v_000938', 'v_000939', 'v_000940', 'v_000941', 'v_000942', 'v_000943', 'v_000944', 'v_000945', 'v_000946', 'v_000947', 'v_000948', 'v_000949', 'v_000985', 'v_000986', 'v_000987', 'v_000988', 'v_000989', 'v_000990', 'v_000991', 'v_000992', 'v_000993', 'v_000994', 'v_000995', 'v_000996', 'v_000997', 'v_000998', 'v_000999', 'v_001000', 'v_001001', 'v_001002', 'v_001003', 'v_001004', 'v_001005', 'v_001006', 'v_001007', 'v_001008', 'v_001009', 'v_001010', 'v_001011', 'v_001012', 'v_001013', 'v_001014', 'v_001015', 'v_001016', 'v_001017', 'v_001018', 'v_001019', 'v_001020', 'v_001021', 'v_001022', 'v_001023', 'v_001024', 'v_001025', 'v_001026', 'v_001027', 'v_001028', 'v_001029', 'v_001030', 'v_001031', 'v_001032', 'v_001033', 'v_001034', 'v_001035', 'v_001036', 'v_001037', 'v_001038', 'v_001039', 'v_001040', 'v_001041', 'v_001042', 'v_001043', 'v_001044', 'v_001045', 'v_001046', 'v_001047', 'v_001048', 'v_001049', 'v_001050', 'v_001051', 'v_001052', 'v_001053', 'v_001054', 'v_001055', 'v_001056', 'v_001057', 'v_001058', 'v_001059', 'v_001060', 'v_001061', 'v_001062', 'v_001063', 'v_001064', 'v_001065', 'v_001066', 'v_001067', 'v_001068', 'v_001069', 'v_001070', 'v_001071', 'v_001072', 'v_001073', 'v_001074', 'v_001075', 'v_001076', 'v_001077', 'v_001078', 'v_001079', 'v_001080', 'v_001081', 'v_001082', 'v_001083', 'v_001121', 'v_001122', 'v_001123', 'v_001124', 'v_001125', 'v_001126', 'v_001127', 'v_001128', 'v_001129', 'v_001130', 'v_001131', 'v_001132', 'v_001133', 'v_001134', 'v_001135', 'v_001136', 'v_001137', 'v_001138', 'v_001139', 'v_001140', 'v_001141', 'v_001142', 'v_001143', 'v_001144', 'v_001145', 'v_001146', 'v_001147', 'v_001148', 'v_001149', 'v_001150', 'v_001151', 'v_001152', 'v_001153', 'v_001154', 'v_001155', 'v_001156', 'v_001157', 'v_001158', 'v_001159', 'v_001160', 'v_001161', 'v_001162', 'v_001163', 'v_001164', 'v_001165', 'v_001166', 'v_001167', 'v_001168', 'v_001169', 'v_001170', 'v_001171', 'v_001172', 'v_001173', 'v_001174', 'v_001175', 'v_001176', 'v_001177', 'v_001178', 'v_001179', 'v_001180', 'v_001181', 'v_001182', 'v_001183', 'v_001184', 'v_001185', 'v_001186', 'v_001187', 'v_001188', 'v_001189', 'v_001190', 'v_001191', 'v_001192', 'v_001193', 'v_001194', 'v_001195', 'v_001196', 'v_001197', 'v_001198', 'v_001199', 'v_001200', 'v_001201', 'v_001202', 'v_001203', 'v_001204', 'v_001205', 'v_001206', 'v_001207', 'v_001208', 'v_001209', 'v_001210', 'v_001211', 'v_001212', 'v_001213', 'v_001214', 'v_001263', 'v_001264', 'v_001265', 'v_001266', 'v_001267', 'v_001268', 'v_001269', 'v_001270', 'v_001271', 'v_001272', 'v_001273', 'v_001274', 'v_001275', 'v_001276', 'v_001277', 'v_001278', 'v_001279', 'v_001280', 'v_001281', 'v_001282', 'v_001283', 'v_001284', 'v_001285', 'v_001286', 'v_001287', 'v_001288', 'v_001289', 'v_001290', 'v_001291', 'v_001292', 'v_001293', 'v_001294', 'v_001295', 'v_001296', 'v_001297', 'v_001298', 'v_001299', 'v_001300', 'v_001301', 'v_001302', 'v_001303', 'v_001304', 'v_001305', 'v_001306', 'v_001307', 'v_001308', 'v_001309', 'v_001310', 'v_001311', 'v_001312', 'v_001313', 'v_001314', 'v_001315', 'v_001316', 'v_001317', 'v_001318', 'v_001319', 'v_001320', 'v_001321', 'v_001322', 'v_001323', 'v_001324', 'v_001325', 'v_001326', 'v_001327', 'v_001328', 'v_001329', 'v_001330', 'v_001331', 'v_001332', 'v_001333', 'v_001334', 'v_001335', 'v_001336', 'v_001337', 'v_001338', 'v_001339', 'v_001340', 'v_001341', 'v_001342', 'v_001343', 'v_001344', 'v_001345', 'v_001346', 'v_001347', 'v_001348', 'v_001349', 'v_001350', 'v_001351', 'v_001352', 'v_001353', 'v_001354', 'v_001355', 'v_001356', 'v_001357', 'v_001358', 'v_001359', 'v_001360', 'v_001361', 'v_001362', 'v_001363', 'v_001364', 'v_001365', 'v_001366', 'v_001367', 'v_001368', 'v_001369', 'v_001370', 'v_001371', 'v_001372', 'v_001373', 'v_001374', 'v_001413', 'v_001414', 'v_001415', 'v_001416', 'v_001417', 'v_001418', 'v_001419', 'v_001420', 'v_001421', 'v_001422', 'v_001423', 'v_001424', 'v_001425', 'v_001426', 'v_001427']\n","[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Eki2hPIS1N4W","colab_type":"code","outputId":"a898dbb7-e12d-4b27-ce9b-ef2df4bae15b","executionInfo":{"status":"ok","timestamp":1574479041093,"user_tz":300,"elapsed":583,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["test25_folders1 = test25_folders[:500]\n","test25_folders2 = test25_folders[500:]\n","\n","test25_labels1 = test25_labels[:500]\n","test25_labels2 = test25_labels[500:]\n","\n","print(len(test25_labels2))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["451\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hR3E-yaz16HX","colab_type":"code","colab":{}},"source":["\"\"\"\n","import time\n","import gc\n","gc.collect()\n","if __name__ == '__main__':\n","  torch.cuda.empty_cache()\n","  #kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","  test25_object1 = UCF101Dataset(test25_folders1, test25_labels1, './images/', torchvision.models.vgg16(True))\n","  test25_dataloader1 = DataLoader(test25_object1, batch_size=25,\n","                        shuffle=False, num_workers = 0, pin_memory = True)\n","  i = 0\n","\n","  start = time.time()\n","  for a in test25_dataloader1:\n","        print(a)\n","        i += 1      \n","        print(\"Folders done\", i)\n","        print(time.time() - start)\n","\"\"\"        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jc-FMc0nsUi9","colab_type":"code","colab":{}},"source":["\"\"\"\n","\n","import time\n","import gc\n","gc.collect()\n","if __name__ == '__main__':\n","  torch.cuda.empty_cache()\n","  #kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","  test25_object2 = UCF101Dataset(test25_folders2, test25_labels2, './images/', torchvision.models.vgg16(True))\n","  test25_dataloader2 = DataLoader(test25_object2, batch_size=25,\n","                        shuffle=False, num_workers = 0, pin_memory = True)\n","  i = 0\n","\n","  start = time.time()\n","  for a in test25_dataloader2:\n","        print(a)\n","        i += 1      \n","        print(\"Folders done\", i)\n","        print(time.time() - start)\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","import pickle              # import module first\n","\n","f = open('test25_dataloader2.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(test25_dataloader2, f, -1)          # dump data to f\n","f.close()\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9c4aZh6DsAMS","colab_type":"code","colab":{}},"source":["\"\"\"\n","import pickle              # import module first\n","\n","f = open('test25_dataloader1.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(test25_dataloader1, f, -1)          # dump data to f\n","f.close()\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VUNtOLwb1E0W","colab_type":"code","colab":{}},"source":["\"\"\"\n","#del vgg16.classifier[1:]\n","import time\n","\n","if __name__ == '__main__':\n","  torch.cuda.empty_cache()\n","  #kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","  train25_object1 = UCF101Dataset(train25_folders1, train25_labels1, './images/', torchvision.models.vgg16(True))\n","  train25_dataloader1 = DataLoader(train25_object1, batch_size=1250,\n","                        shuffle=False, num_workers = 0, pin_memory = True)\n","  i = 0\n","\n","  start = time.time()\n","  for a in train25_dataloader1:\n","        print(a)\n","        i += 50      \n","        print(\"Folders done\", i)\n","        print(time.time() - start)\n","\"\"\"        \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lIFMe1w9DsGM","colab_type":"code","colab":{}},"source":["\"\"\"\n","if __name__ == '__main__':\n","  torch.cuda.empty_cache()\n","  #kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","  train25_object2 = UCF101Dataset(train25_folders2, train25_labels2, './images/', torchvision.models.vgg16(True))\n","  train25_dataloader2 = DataLoader(train25_object2, batch_size=1250,\n","                        shuffle=False, num_workers = 0, pin_memory = True)\n","  i = 500\n","\n","  start = time.time()\n","  for _, a in enumerate(train25_dataloader2):\n","        print(a)\n","        i += 50      \n","        print(\"Folders done\", i)\n","        print(time.time() - start)\n","\"\"\"        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8zquL4djSWm","colab_type":"code","colab":{}},"source":["torch.cuda.empty_cache()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b9VklvoyPa61","colab_type":"code","colab":{}},"source":["\"\"\"\n","if __name__ == '__main__':\n","  torch.cuda.empty_cache()\n","  #kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","  train25_object3 = UCF101Dataset(train25_folders3, train25_labels3, './images/', torchvision.models.vgg16(True))\n","  train25_dataloader3 = DataLoader(train25_object3, batch_size=1250,\n","                        shuffle=False, num_workers = 0, pin_memory = True)\n","  i = 1000\n","\n","  start = time.time()\n","  for _, a in enumerate(train25_dataloader3):\n","        print(a)\n","        i += 50      \n","        print(\"Folders done\", i)\n","        print(time.time() - start)\n","\"\"\"\n","             "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MPQK_8ghAJJx","colab_type":"code","colab":{}},"source":["\"\"\"\n","import pickle              # import module first\n","\n","f = open('train25_dataloader3.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(train25_dataloader3, f, -1)          # dump data to f\n","f.close()\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hdILUa8Soa0","colab_type":"code","colab":{}},"source":["\"\"\"\n","if __name__ == '__main__':\n","\n","  torch.cuda.empty_cache()\n","  #kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","  train25_object4 = UCF101Dataset(train25_folders4, train25_labels4, './images/', torchvision.models.vgg16(True))\n","  train25_dataloader4 = DataLoader(train25_object4, batch_size=1250,\n","                        shuffle=False, num_workers = 0, pin_memory = True)\n","  i = 1500\n","\n","  start = time.time()\n","  for _, b in enumerate(train25_dataloader4):\n","        print(b)\n","        i += 50      \n","        print(\"Folders done\", i)\n","        print(time.time() - start)\n","\n"," \"\"\" "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qyFfxh9tSr_h","colab_type":"code","outputId":"63c531db-c1f3-4851-c659-945b9a5c32fa","executionInfo":{"status":"error","timestamp":1574354044604,"user_tz":300,"elapsed":1490631,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\"\"\"\n","#del vgg16.classifier[1:]\n","#import time\n","\n","if __name__ == '__main__':\n","  torch.cuda.empty_cache()\n","  #kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","  train25_object5 = UCF101Dataset(train25_folders5, train25_labels5, './images/', torchvision.models.vgg16(True))\n","  train25_dataloader5_1 = DataLoader(train25_object5, batch_size=1,\n","                        shuffle=False, num_workers = 0, pin_memory = True)\n","  i = 0\n","\n","  start = time.time()\n","  for a in train25_dataloader5_1:\n","        gc.collect()\n","        #print(a)\n","        i += 1      \n","        print(\"Folders done\", i, time.time() - start)\n","        #gc.collect()\n","        torch.cuda.empty_cache()\n","        #print(time.time() - start)\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","import pickle              # import module first\n","\n","f = open('train25_dataloader5_1.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(train25_dataloader5_1, f, -1)          # dump data to f\n","f.close()\n","\"\"\"    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n","Sequential(\n","  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",")\n","Folders done 1 2.7607970237731934\n","Folders done 2 5.534983158111572\n","Folders done 3 14.899595022201538\n","Folders done 4 27.64732551574707\n","Folders done 5 38.276832580566406\n","Folders done 6 49.93973398208618\n","Folders done 7 61.76861000061035\n","Folders done 8 73.76342463493347\n","Folders done 9 93.53428721427917\n","Folders done 10 105.90463781356812\n","Folders done 11 118.29985117912292\n","Folders done 12 128.79759335517883\n","Folders done 13 140.00290870666504\n","Folders done 14 150.9299876689911\n","Folders done 15 161.69674062728882\n","Folders done 16 172.02588510513306\n","Folders done 17 182.19706535339355\n","Folders done 18 194.6644332408905\n","Folders done 19 205.78002738952637\n","Folders done 20 216.78631496429443\n","Folders done 21 228.39944052696228\n","Folders done 22 239.98511481285095\n","Folders done 23 251.45474576950073\n","Folders done 24 263.0483546257019\n","Folders done 25 273.57896065711975\n","Folders done 26 286.6323256492615\n","Folders done 27 298.55090284347534\n","Folders done 28 310.2121934890747\n","Folders done 29 322.1455125808716\n","Folders done 30 333.00013875961304\n","Folders done 31 344.4213819503784\n","Folders done 32 355.29452538490295\n","Folders done 33 366.4399902820587\n","Folders done 34 377.73410987854004\n","Folders done 35 388.46906781196594\n","Folders done 36 398.05690574645996\n","Folders done 37 410.60926818847656\n","Folders done 38 422.78753542900085\n","Folders done 39 432.89594864845276\n","Folders done 40 444.03297662734985\n","Folders done 41 454.33808422088623\n","Folders done 42 466.17140078544617\n","Folders done 43 478.4430105686188\n","Folders done 44 490.1977083683014\n","Folders done 45 501.33123779296875\n","Folders done 46 511.60981035232544\n","Folders done 47 523.2233521938324\n","Folders done 48 535.8860409259796\n","Folders done 49 546.9701805114746\n","Folders done 50 558.3335404396057\n","Folders done 51 569.9103689193726\n","Folders done 52 581.4464521408081\n","Folders done 53 592.3705780506134\n","Folders done 54 604.8651156425476\n","Folders done 55 615.5476689338684\n","Folders done 56 626.1236407756805\n","Folders done 57 636.8631930351257\n","Folders done 58 647.9169063568115\n","Folders done 59 659.0579946041107\n","Folders done 60 669.6757066249847\n","Folders done 61 680.331910610199\n","Folders done 62 690.4811723232269\n","Folders done 63 703.0181283950806\n","Folders done 64 714.3694965839386\n","Folders done 65 725.1417949199677\n","Folders done 66 736.6739394664764\n","Folders done 67 748.2273120880127\n","Folders done 68 758.885989189148\n","Folders done 69 768.8347346782684\n","Folders done 70 779.9788482189178\n","Folders done 71 790.5362906455994\n","Folders done 72 802.794903755188\n","Folders done 73 813.3401598930359\n","Folders done 74 824.1371872425079\n","Folders done 75 835.9599046707153\n","Folders done 76 848.7690839767456\n","Folders done 77 860.1071829795837\n","Folders done 78 870.3242890834808\n","Folders done 79 880.2783074378967\n","Folders done 80 892.2777152061462\n","Folders done 81 903.0127518177032\n","Folders done 82 914.0066158771515\n","Folders done 83 924.5101754665375\n","Folders done 84 935.1057653427124\n","Folders done 85 946.9145686626434\n","Folders done 86 957.2537813186646\n","Folders done 87 967.8925836086273\n","Folders done 88 978.9407632350922\n","Folders done 89 990.0955727100372\n","Folders done 90 1000.5772984027863\n","Folders done 91 1012.1522512435913\n","Folders done 92 1022.9473969936371\n","Folders done 93 1034.7532382011414\n","Folders done 94 1045.6214408874512\n","Folders done 95 1057.1120240688324\n","Folders done 96 1067.4161188602448\n","Folders done 97 1078.8384277820587\n","Folders done 98 1089.8720560073853\n","Folders done 99 1103.1312413215637\n","Folders done 100 1114.080239534378\n","Folders done 101 1125.107396364212\n","Folders done 102 1136.4179890155792\n","Folders done 103 1147.3099806308746\n","Folders done 104 1158.2601368427277\n","Folders done 105 1168.9406895637512\n","Folders done 106 1180.5721192359924\n","Folders done 107 1193.7378180027008\n","Folders done 108 1204.84250998497\n","Folders done 109 1219.9753155708313\n","Folders done 110 1231.330199956894\n","Folders done 111 1244.112489938736\n","Folders done 112 1255.7496058940887\n","Folders done 113 1267.6400949954987\n","Folders done 114 1278.1301126480103\n","Folders done 115 1289.6054668426514\n","Folders done 116 1301.3842720985413\n","Folders done 117 1313.7011964321136\n","Folders done 118 1329.652511358261\n","Folders done 119 1340.7227067947388\n","Folders done 120 1351.836404800415\n","Folders done 121 1365.7990090847015\n","Folders done 122 1384.7865488529205\n","Folders done 123 1396.7150220870972\n","Folders done 124 1408.0961406230927\n","Folders done 125 1418.1748535633087\n","Folders done 126 1428.259433746338\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-d059e51b4ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain25_dataloader5_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-f3ffea1ceb8f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#for i in range(len(files)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: './images/v_002996'"]}]},{"cell_type":"code","metadata":{"id":"6gCsj8i5U3m5","colab_type":"code","colab":{}},"source":["\"\"\"\n","import pickle              # import module first\n","\n","f = open('train25_dataloader5_10folders.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(train25_dataloader5, f, -1)          # dump data to f\n","f.close()\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fa9EVNKMTu8-","colab_type":"code","colab":{}},"source":["\"\"\"\n","import pickle\n","\n","f = open('train25_dataloader1.pkl', 'rb')   # 'rb' for reading binary file\n","train25_dataloader1_pkl = pickle.load(f)     \n","f.close() \n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4y4wJ4qVRKS","colab_type":"code","outputId":"80392cde-1096-49d9-e5e1-f510c421b163","executionInfo":{"status":"ok","timestamp":1574282408493,"user_tz":300,"elapsed":22274,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","\"\"\"\n","f = open('train25_dataloader2.pkl', 'rb')   # 'rb' for reading binary file\n","train25_dataloader2_pkl = pickle.load(f)     \n","f.close() \n","\n","f = open('train25_dataloader3.pkl', 'rb')   # 'rb' for reading binary file\n","train25_dataloader3_pkl = pickle.load(f)     \n","f.close() \n","\n","f = open('train25_dataloader4.pkl', 'rb')   # 'rb' for reading binary file\n","train25_dataloader4_pkl = pickle.load(f)     \n","f.close() \n","\n","f = open('train25_dataloader5_10folders.pkl', 'rb')   # 'rb' for reading binary file\n","train25_dataloader5_10_pkl = pickle.load(f)     \n","f.close()                       \n","\n","print(len(train25_dataloader3_pkl))\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sw4_gT3-S8zy","colab_type":"code","outputId":"d38d856e-9b0f-43e2-a474-15747bc5c1e1","executionInfo":{"status":"ok","timestamp":1574350164814,"user_tz":300,"elapsed":617,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import gc\n","gc.collect()\n","#torch.cuda.empty_cache()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Wid7MTq4NuFe","colab_type":"code","outputId":"4c5302f9-6d01-49cc-f2ac-93081cc812c2","executionInfo":{"status":"ok","timestamp":1574262153668,"user_tz":300,"elapsed":2797,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\"\n","train25_dataloader5_50_trial = DataLoader(train25_dataloader5_10_pkl, batch_size=5,\n","                        shuffle=False, num_workers = 0, pin_memory = True)\n","\n","f = open('train25_dataloader5_50_trial.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(train25_dataloader5_50_trial, f, -1)          # dump data to f\n","f.close()\n","\n","print(len(train25_dataloader5_50_trial))\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5VCgngDNOQ0n","colab_type":"code","colab":{}},"source":["\"\"\"\n","f = open('train25_dataloader5_50_trial.pkl', 'rb')   # 'rb' for reading binary file\n","train25_dataloader5_50_trial_pkl = pickle.load(f)     \n","f.close() \n","\"\"\"  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PTTIqk85QlSA","colab_type":"code","colab":{}},"source":["#print(train25_object5[1000])\n","features25_list = []\n","labels_list_tot = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2SziBl6M0bw","colab_type":"code","outputId":"57dd9109-13b2-42a5-8c08-82f3b2abdca7","executionInfo":{"status":"ok","timestamp":1574347315375,"user_tz":300,"elapsed":606801,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["\"\"\"\n","j = 0\n","for feats, labels in train25_dataloader1_pkl:\n","  #print(feats.size())\n","  i = 0\n","  #j =\n","  while i < (feats.size()[0]//25):\n","    feat_reshaped = torch.reshape(feats[25*i:25*(i+1)], (-1, 102400))\n","    labels_reshaped = torch.reshape(labels[25*i:25*(i+1)], (-1, 25))\n","    features25_list.append(feat_reshaped[0])\n","    labels_list_tot.append(labels_reshaped[0][0]) \n","    #print(feat_reshaped.size(), labels_reshaped.size())\n","    #print(feat_reshaped, labels_reshaped[0][0])\n","    #print(len(features25_list), len(features25_list[0]))\n","    i += 1\n","    gc.collect()\n","\n","  j += 50\n","  print(j, len(features25_list))\n","  \"\"\"\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["50 50\n","100 100\n","150 150\n","200 200\n","250 250\n","300 300\n","350 350\n","400 400\n","450 450\n","500 500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uR2psMYHtne6","colab_type":"code","outputId":"202669d8-d07f-40e7-c2bd-0a256e8f6448","executionInfo":{"status":"ok","timestamp":1574347480399,"user_tz":300,"elapsed":468,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#import gc\n","gc.collect()\n","#torch.cuda.empty_cache()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"WMxRLZnAibPW","colab_type":"code","colab":{}},"source":["\"\"\"\n","import pickle\n","\n","f = open('features25_list_1.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(features25_list, f, -1)          # dump data to f\n","f.close()\n","\"\"\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6635onxhuQyP","colab_type":"code","outputId":"0f860dbd-c3ca-425b-b4be-4d1469f95396","executionInfo":{"status":"ok","timestamp":1574350582213,"user_tz":300,"elapsed":409,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["gc.collect()\n","#torch.cuda.empty_cache()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"NN6bs8iKSgFs","colab_type":"code","colab":{}},"source":["\"\"\"\n","f = open('labels_list_tot_1.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(labels_list_tot, f, -1)          # dump data to f\n","f.close()\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kme9D84nK5zw","colab_type":"code","colab":{}},"source":["#features25_list_1_pkl = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybHjeU_OcNSq","colab_type":"code","colab":{}},"source":["\"\"\"\n","import pickle\n","\n","f = open('features25_list_4.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_3_pkl = pickle.load(f)     \n","f.close()\n","\"\"\" "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIROk3DOdpXj","colab_type":"code","colab":{}},"source":["\"\"\"\n","f = open('labels_list_tot_4.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_3_pkl = pickle.load(f)     \n","f.close() \n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"swPRYftCcngy","colab_type":"code","outputId":"d1cf67bf-7835-4e7f-ad11-a8bf03cba919","executionInfo":{"status":"ok","timestamp":1574350586318,"user_tz":300,"elapsed":837,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["gc.collect()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"BU8rh1Hl__Pp","colab_type":"code","colab":{}},"source":["\"\"\"\n","for feats, labels in train25_dataloader2_pkl:\n","  #print(feats.size())\n","  i = 0\n","  while i < (feats.size()[0]//25):\n","    feat_reshaped = torch.reshape(feats[25*i:25*(i+1)], (-1, 102400))\n","    labels_reshaped = torch.reshape(labels[25*i:25*(i+1)], (-1, 25))\n","    features25_list.append(feat_reshaped[0])\n","    labels_list_tot.append(labels_reshaped[0][0]) \n","    #print(feat_reshaped.size(), labels_reshaped.size())\n","    #print(feat_reshaped, labels_reshaped[0][0])\n","    #print(len(features25_list), len(features25_list[0]))\n","    i += 1\n","  print(j+50, len(features25_list))\n","  f = open('features25_list.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","  pickle.dump(features25_list, f, -1)          # dump data to f\n","  f.close()\n","\n","  f = open('labels_list_tot.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","  pickle.dump(labels_list_tot, f, -1)          # dump data to f\n","  f.close()\n","\"\"\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vl1fhNpXAFzR","colab_type":"code","colab":{}},"source":["\"\"\"\n","for feats, labels in train25_dataloader3_pkl:\n","  #print(feats.size())\n","  i = 0\n","  while i < (feats.size()[0]//25):\n","    feat_reshaped = torch.reshape(feats[25*i:25*(i+1)], (-1, 102400))\n","    labels_reshaped = torch.reshape(labels[25*i:25*(i+1)], (-1, 25))\n","    features25_list.append(feat_reshaped[0])\n","    labels_list_tot.append(labels_reshaped[0][0]) \n","    #print(feat_reshaped.size(), labels_reshaped.size())\n","    #print(feat_reshaped, labels_reshaped[0][0])\n","    #print(len(features25_list), len(features25_list[0]))\n","    i += 1\n","  print(j+50, len(features25_list))  \n","  f = open('features25_list.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","  pickle.dump(features25_list, f, -1)          # dump data to f\n","  f.close()\n","\n","  f = open('labels_list_tot.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","  pickle.dump(labels_list_tot, f, -1)          # dump data to f\n","  f.close()  \n","\"\"\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMN0h_6XAICW","colab_type":"code","colab":{}},"source":["\"\"\"\n","for feats, labels in train25_dataloader4_pkl:\n","  #print(feats.size())\n","  i = 0\n","  while i < (feats.size()[0]//25):\n","    feat_reshaped = torch.reshape(feats[25*i:25*(i+1)], (-1, 102400))\n","    labels_reshaped = torch.reshape(labels[25*i:25*(i+1)], (-1, 25))\n","    features25_list.append(feat_reshaped[0])\n","    labels_list_tot.append(labels_reshaped[0][0]) \n","    #print(feat_reshaped.size(), labels_reshaped.size())\n","    #print(feat_reshaped, labels_reshaped[0][0])\n","    #print(len(features25_list), len(features25_list[0]))\n","    i += 1\n","  print(j+50, len(features25_list))  \n","  f = open('features25_list.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","  pickle.dump(features25_list, f, -1)          # dump data to f\n","  f.close()\n","\n","  f = open('labels_list_tot.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","  pickle.dump(labels_list_tot, f, -1)          # dump data to f\n","  f.close()     \n","\"\"\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjbNFh3VAJov","colab_type":"code","colab":{}},"source":["\"\"\"\n","for feats, labels in train25_dataloader5_10_pkl:\n","  #print(feats.size())\n","  i = 0\n","  while i < (feats.size()[0]//25):\n","    feat_reshaped = torch.reshape(feats[25*i:25*(i+1)], (-1, 102400))\n","    labels_reshaped = torch.reshape(labels[25*i:25*(i+1)], (-1, 25))\n","    features25_list.append(feat_reshaped[0])\n","    labels_list_tot.append(labels_reshaped[0][0]) \n","    #print(feat_reshaped.size(), labels_reshaped.size())\n","    #print(feat_reshaped, labels_reshaped[0][0])\n","    #print(len(features25_list), len(features25_list[0]))\n","    i += 1\n","  print(j+10, len(features25_list))  \n","  f = open('features25_list.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","  pickle.dump(features25_list, f, -1)          # dump data to f\n","  f.close()\n","\n","  f = open('labels_list_tot.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","  pickle.dump(labels_list_tot, f, -1)          # dump data to f\n","  f.close()\n","\n","print(len(features25_list))\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJ5i1uVS87Sz","colab_type":"code","colab":{}},"source":["\"\"\"\n","f = open('features25_list.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(features25_list, f, -1)          # dump data to f\n","f.close()\n","\n","f = open('labels_list_tot.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","pickle.dump(labels_list_tot, f, -1)          # dump data to f\n","f.close()\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQkxkB8Yazn4","colab_type":"code","outputId":"61b5d081-4204-4814-b69c-8bea0a5b3ae3","executionInfo":{"status":"ok","timestamp":1574347489158,"user_tz":300,"elapsed":382,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["gc.collect()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"urKQi8oAjYb-","colab":{}},"source":["# \\*write your codes for feature extraction (You can use multiple cells, this is just a place holder)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UN74WLWpl7zQ"},"source":["***\n","***\n","## **Problem 2.** Modelling"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Do5OSV9Mnmwy"},"source":["* ##### **Print the size of your training and test data**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uqshyjO3mHkt","colab":{}},"source":["class LSTM(nn.Module):\n","    \n","    def __init__(self, hidden_dim, batch_size, label_size):\n","        super(LSTM, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.batch_size= batch_size\n","        \n","        self.lstm = nn.LSTM(102400, hidden_dim)\n","        self.hidden2label = nn.Linear(hidden_dim, label_size)\n","        self.hidden = self.init_hidden()\n","    \n","    def init_hidden(self):\n","        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n","                torch.zeros(1, self.batch_size, self.hidden_dim))\n","    \n","    def forward(self, input_features):\n","        inputs = input_features.view(self.batch_size,-1)\n","        lstm_out, self.hidden = self.lstm(inputs.view(1,self.batch_size,-1), self.hidden)\n","        outputs = self.hidden2label(lstm_out.view(self.batch_size,-1))\n","        output_labels = F.log_softmax(outputs, dim=-1)\n","        #print(output_labels.shape)\n","        \n","        return output_labels\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"saul1e3jf1_H","colab_type":"code","outputId":"814155c8-8cc5-4a5b-8ec4-d0cc3aaef7d5","executionInfo":{"status":"ok","timestamp":1574408289545,"user_tz":600,"elapsed":14398,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pickle\n","\n","f = open('features25_list_5.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_5_pkl = pickle.load(f)     \n","f.close() \n","\n","print(len(features25_list_5_pkl))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["409\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fnb0EEhxoZiw","colab_type":"code","colab":{}},"source":["f = open('labels_list_tot_5.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_5_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0EGU31IJn5_h","outputId":"d2e97619-af6e-472b-87d4-2a1af076ea2a","executionInfo":{"status":"ok","timestamp":1574409825982,"user_tz":600,"elapsed":239,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Don't hardcode the shape of train and test data\n","print('Shape of training data is :', len(train25_labels),\"x\", len(features25_list_5_pkl[0]))\n","print('Shape of test/validation data is :', len(test25_labels), \"x\", len(features25_list_5_pkl[0]))\n","\n","#I had divided the complete dataset into 5 parts to parallely extract features initially "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape of training data is : 2409 x 102400\n","Shape of test/validation data is : 951 x 102400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ur6sVIp90DWd","colab_type":"code","colab":{}},"source":["from sklearn.utils import shuffle\n","\n","#features25_list_2_pkl_shuffled, labels_list_tot_2_pkl_shuffled  = shuffle(features25_list_2_pkl, labels_list_tot_2_pkl)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JQoPmDVqiQw","colab_type":"code","outputId":"fbef931a-321a-4c1c-ac14-4c492eca05c9","executionInfo":{"status":"ok","timestamp":1574406864595,"user_tz":600,"elapsed":255,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["gc.collect()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"Vovvaq-JGd2z","colab_type":"code","outputId":"51e262c8-f685-498e-a0a1-066e9acccc44","executionInfo":{"status":"ok","timestamp":1574408427100,"user_tz":600,"elapsed":109068,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":439}},"source":["#Training each part of data separately\n","#Completele training is also shown below\n","#This is for network with 512 hidden dimensions\n","\n","Batch_size = 50\n","Hidden_dim = 512\n","Num_classes = 25\n","\n","train_data = features25_list_5_pkl\n","train_labels = labels_list_tot_5_pkl\n","\n","state_dict = torch.load('feat512_1234.pth')\n","\n","model_1 = LSTM(Hidden_dim,Batch_size,Num_classes)\n","model_1.load_state_dict(state_dict)\n","print(model_1)\n","loss_function = nn.NLLLoss()\n","optimizer = optim.SGD(model_1.parameters(), lr=0.1)\n","\n","#model_1 = model_1.to(device) \n","\n","\n","correct = 0\n","train_accuracy = 0\n","for epoch in range(10):\n","    loss_count = 0.0\n","    correct = 0\n","    total = 0\n","    train_accuracy = 0\n","    start = time.time()\n","    #for feats, labels in train25_dataloader2_1_pkl:\n","    for i in range(len(train_data)//Batch_size):\n","      #print(feats.size())\n","      #i = 0\n","      #while i < (feats.size()[0]//25):\n","      #  feat_reshaped = torch.reshape(feats[25*i:25*(i+1)], (-1, 102400))\n","      #  labels_reshaped = torch.reshape(labels[25*i:25*(i+1)], (-1, 25))\n","        #print(feat_reshaped.size(), labels_reshaped.size())\n","        #print(feat_reshaped, labels_reshaped[0])\n","        #feats = input_data[i]\n","        #labels = label[i]\n","      inputs  = train_data[i: i+Batch_size]\n","      inputs = torch.stack(inputs)\n","      #inputs = inputs.to(device)\n","      labels = train_labels[i : i+Batch_size]\n","      labels = torch.stack(labels)\n","      #print(labels.shape)\n","      #labels = labels.to(device)\n","      model_1.zero_grad()\n","      model_1.hidden= model_1.init_hidden()\n","      outputs = model_1(inputs)\n","      #print(outputs.shape, labels.shape)\n","      loss = loss_function(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","      loss_count += loss.item()\n","      _, predicted = torch.max(outputs.data, 1)\n","      \n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","      del outputs, inputs, labels\n","      #torch.cuda.empty_cache()\n","      train_accuracy = (correct/total)*100\n","      \n","      #i += 1 \n","\n","    print('Epoch : %d Loss: %.3f' %(epoch+1, loss_count/len(features25_list_5_pkl)))\n","    print('Training Accuracy', train_accuracy , time.time() - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LSTM(\n","  (lstm): LSTM(102400, 512)\n","  (hidden2label): Linear(in_features=512, out_features=25, bias=True)\n",")\n","Epoch : 1 Loss: 0.026\n","Training Accuracy 71.75 10.44509768486023\n","Epoch : 2 Loss: 0.002\n","Training Accuracy 100.0 10.295786619186401\n","Epoch : 3 Loss: 0.001\n","Training Accuracy 100.0 10.450769424438477\n","Epoch : 4 Loss: 0.000\n","Training Accuracy 100.0 10.509390354156494\n","Epoch : 5 Loss: 0.000\n","Training Accuracy 100.0 10.59251356124878\n","Epoch : 6 Loss: 0.000\n","Training Accuracy 100.0 10.676548957824707\n","Epoch : 7 Loss: 0.000\n","Training Accuracy 100.0 10.7195143699646\n","Epoch : 8 Loss: 0.000\n","Training Accuracy 100.0 10.695135593414307\n","Epoch : 9 Loss: 0.000\n","Training Accuracy 100.0 10.703275918960571\n","Epoch : 10 Loss: 0.000\n","Training Accuracy 100.0 10.739113807678223\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P-gVsMvRXGPv","colab_type":"code","colab":{}},"source":["torch.save(model_1.state_dict(), 'feat512_12345.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v0znm2TMmsDZ"},"source":["---\n","---\n","## **Problem 3.** Evaluation"]},{"cell_type":"code","metadata":{"id":"_O1t9OQGP1g1","colab_type":"code","colab":{}},"source":["import pickle\n","\n","f = open('features25_list_test_1.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_test_1_pkl = pickle.load(f)     \n","f.close() \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1L5udtxWci_","colab_type":"code","outputId":"babf8864-b0e1-4a13-a360-91a6ce907867","executionInfo":{"status":"ok","timestamp":1574479272878,"user_tz":300,"elapsed":1320,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(len(features25_list_test_1_pkl))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7SfeBrqAVsqs","colab_type":"code","colab":{}},"source":["f = open('features25_list_test_2.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_test_2_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zi0CpdvXQr6","colab_type":"code","colab":{}},"source":["test_data = features25_list_test_1_pkl + features25_list_test_2_pkl\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BljpjLHZWi1-","colab_type":"code","colab":{}},"source":["print(len(features25_list_test_2_pkl))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDXgCtT9P5kr","colab_type":"code","outputId":"b4dce422-e820-4c8a-e038-d10684a8f532","executionInfo":{"status":"ok","timestamp":1574479426776,"user_tz":300,"elapsed":1170,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["gc.collect()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"we29qeKGP5C1","colab_type":"code","colab":{}},"source":["f = open('labels_list_tot_test_1.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_test_1_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cBMhwbGXtKl","colab_type":"code","colab":{}},"source":["print(len(labels_list_tot_test_1_pkl))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XjmmGtiVyI6","colab_type":"code","colab":{}},"source":["f = open('labels_list_tot_test_2.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_test_2_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PVHKsJ6V4Tw","colab_type":"code","colab":{}},"source":["test_labels = labels_list_tot_test_1_pkl + labels_list_tot_test_2_pkl\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5lO0y0CahOL","colab_type":"code","colab":{}},"source":["#del features25_list_test_1_pkl, features25_list_test_2_pkl, test_data\n","#del labels_list_tot_test_1_pkl, labels_list_tot_test_2_pkl, test_labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlK1unmhP951","colab_type":"code","colab":{}},"source":["from sklearn.utils import shuffle\n","\n","#features25_list_test_1_pkl_shuffled, labels_list_tot_test_1_pkl_shuffled  = shuffle(features25_list_test_1_pkl, labels_list_tot_test_1_pkl)\n","test_data = features25_list_test_1_pkl \n","test_labels = labels_list_tot_test_1_pkl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ARtMhcbXmsXk","outputId":"9ddd02b1-02e3-48d5-92a3-453967bc81cd","executionInfo":{"status":"ok","timestamp":1574410032984,"user_tz":600,"elapsed":3395,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import time\n","test_accuracy = 0\n","\n","Batch_size = 50\n","Hidden_dim = 256\n","Num_classes = 25\n","\n","correct2 = 0\n","total2 = 0\n","\n","state_dict = torch.load('feat5_4_3_s_2_s_1_s.pth')\n","\n","model = LSTM(Hidden_dim,Batch_size,Num_classes)\n","model.load_state_dict(state_dict)\n","#print(model_1)\n","\n","for epoch in range(1):\n","    loss_count = 0.0\n","    correct = 0\n","    total = 0\n","    accuracy = 0\n","    start = time.time()\n","    #for feats, labels in train25_dataloader2_1_pkl:\n","    for i in range(len(test_data)//Batch_size):\n","      \n","      inputs  = test_data[i: i+Batch_size]\n","      inputs = torch.stack(inputs)\n","      #inputs = inputs.to(device)\n","      labels = test_labels[i : i+Batch_size]\n","      labels = torch.stack(labels)\n","      #print(labels.shape)\n","      #labels = labels.to(device)\n","      #model_1.zero_grad()\n","      #model_1.hidden= model_1.init_hidden()\n","      outputs = model(inputs)\n","      #print(outputs.shape, labels.shape)\n","      #loss = loss_function(outputs, labels)\n","      #loss.backward()\n","      #optimizer.step()\n","      #loss_count += loss.item()\n","      _, predicted = torch.max(outputs.data, 1)\n","      \n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","      del outputs, inputs, labels\n","      torch.cuda.empty_cache()\n","      test_accuracy = (correct/total)*100\n","      #print(correct, total)\n","      \n","      #i += 1 \n","\n","    #print('Epoch : %d Loss: %.3f' %(epoch+1, loss_count/len(features25_list_test_1_pkl)))\n","    print('Testing Accuracy', test_accuracy , time.time() - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Testing Accuracy 79.0 1.462536334991455\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kqgJ70yNQuQO","colab_type":"code","colab":{}},"source":["print(len(features25_list_test_1_pkl))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"80ZUeqnGv48f"},"source":["* ##### **Print the train and test accuracy of your model** "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UMMXAjMjv4g5","outputId":"519eae42-b85e-4935-ed25-7ec2a6328374","executionInfo":{"status":"ok","timestamp":1574409489134,"user_tz":600,"elapsed":377,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Don't hardcode the train and test accuracy\n","print('Training accuracy is %2.3f :' %(100.00), train_accuracy )\n","print('Test accuracy is %2.3f :' %(100.00), test_accuracy )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training accuracy is 100.000 : 100.0\n","Test accuracy is 100.000 : 79.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eesNQn6FYKQz"},"source":["* ##### **Print the train and test and test accuracy of SVM** "]},{"cell_type":"code","metadata":{"id":"j__NjaKZJ628","colab_type":"code","colab":{}},"source":["#test\n","test_data = features25_list_test_1_pkl + features25_list_test_2_pkl\n","test_labels = labels_list_tot_test_1_pkl + labels_list_tot_test_2_pkl\n","\n","test_data_numpy = []\n","test_labels_numpy = []\n","\n","for i in range(len(test_data)):\n","  test_data_numpy.append(test_data[i].numpy())\n","  test_labels_numpy.append(test_labels[i].numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCIMpOruTri1","colab_type":"code","colab":{}},"source":["f = open('features25_list_1.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_1_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqekAJF9QZyF","colab_type":"code","colab":{}},"source":["f = open('features25_list_2.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_2_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OTBbPExQlQu","colab_type":"code","colab":{}},"source":["f = open('features25_list_3_1.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_3_1_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-0Co8hkW-Ou","colab_type":"code","colab":{}},"source":["def accuracy_test1(test_labels, pred_labels):\n","  return 2*(test_labels == pred_labels).mean()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAoOaFgMT2U9","colab_type":"code","colab":{}},"source":["f = open('features25_list_4_1.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_4_1_pkl = pickle.load(f)     \n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"405aEAfQUAqY","colab_type":"code","colab":{}},"source":["f = open('features25_list_5.pkl', 'rb')   # 'rb' for reading binary file\n","features25_list_5_pkl = pickle.load(f)     \n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRjRTJKWQqdF","colab_type":"code","colab":{}},"source":["f = open('labels_list_tot_3_1.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_3_1_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WNsrVBuCT7Hv","colab_type":"code","colab":{}},"source":["f = open('labels_list_tot_4_1.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_4_1_pkl = pickle.load(f)     \n","f.close() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fha49Kq4UXeB","colab_type":"code","colab":{}},"source":["f = open('labels_list_tot_1.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_1_pkl = pickle.load(f)     \n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"83K0IiQGQzm_","colab_type":"code","colab":{}},"source":["f = open('labels_list_tot_2.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_2_pkl = pickle.load(f)     \n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BF23tf0mUGUd","colab_type":"code","colab":{}},"source":["f = open('labels_list_tot_5.pkl', 'rb')   # 'rb' for reading binary file\n","labels_list_tot_5_pkl = pickle.load(f)     \n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZuqwzGaKzLi","colab_type":"code","colab":{}},"source":["train_data_full = features25_list_1_pkl + features25_list_2_pkl + features25_list_3_1_pkl + features25_list_4_1_pkl + features25_list_5_pkl \n","train_labels_full = labels_list_tot_1_pkl + labels_list_tot_2_pkl + labels_list_tot_3_1_pkl + labels_list_tot_4_1_pkl + labels_list_tot_5_pkl \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7LzLoOgQ-J0","colab_type":"code","outputId":"dcc7ca00-e864-449b-bccb-3692764b5c64","executionInfo":{"status":"ok","timestamp":1574479992932,"user_tz":300,"elapsed":1720,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(len(train_data_full), len(train_labels_full))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2409 2409\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CU1pqtOwaTp7","colab_type":"code","outputId":"3457903d-d267-40d0-9ba2-529648f41557","executionInfo":{"status":"ok","timestamp":1574482133624,"user_tz":300,"elapsed":177607,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["Batch_size = 50\n","Hidden_dim = 256\n","Num_classes = 25\n","\n","train_data = train_data_full\n","train_labels = train_labels_full\n","\n","state_dict = torch.load('feat5_4_3_s_2_s_1_s.pth')\n","\n","model_1 = LSTM(Hidden_dim,Batch_size,Num_classes)\n","model_1.load_state_dict(state_dict)\n","print(model_1)\n","loss_function = nn.NLLLoss()\n","optimizer = optim.SGD(model_1.parameters(), lr=0.1)\n","\n","#model_1 = model_1.to(device) \n","\n","\n","correct = 0\n","train_accuracy = 0\n","for epoch in range(10):\n","    loss_count = 0.0\n","    correct = 0\n","    total = 0\n","    train_accuracy = 0\n","    start = time.time()\n","    #for feats, labels in train25_dataloader2_1_pkl:\n","    for i in range(len(train_data)//Batch_size):\n","      #print(feats.size())\n","      #i = 0\n","      #while i < (feats.size()[0]//25):\n","      #  feat_reshaped = torch.reshape(feats[25*i:25*(i+1)], (-1, 102400))\n","      #  labels_reshaped = torch.reshape(labels[25*i:25*(i+1)], (-1, 25))\n","        #print(feat_reshaped.size(), labels_reshaped.size())\n","        #print(feat_reshaped, labels_reshaped[0])\n","        #feats = input_data[i]\n","        #labels = label[i]\n","      inputs  = train_data[i: i+Batch_size]\n","      inputs = torch.stack(inputs)\n","      #inputs = inputs.to(device)\n","      labels = train_labels[i : i+Batch_size]\n","      labels = torch.stack(labels)\n","      #print(labels.shape)\n","      #labels = labels.to(device)\n","      model_1.zero_grad()\n","      model_1.hidden= model_1.init_hidden()\n","      outputs = model_1(inputs)\n","      #print(outputs.shape, labels.shape)\n","      loss = loss_function(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","      loss_count += loss.item()\n","      _, predicted = torch.max(outputs.data, 1)\n","      \n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","      del outputs, inputs, labels\n","      #torch.cuda.empty_cache()\n","      train_accuracy = (correct/total)*100\n","      \n","      #i += 1 \n","\n","    print('Epoch : %d Loss: %.3f' %(epoch+1, loss_count/len(train_data)))\n","    print('Training Accuracy', train_accuracy , time.time() - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LSTM(\n","  (lstm): LSTM(102400, 256)\n","  (hidden2label): Linear(in_features=256, out_features=25, bias=True)\n",")\n","Epoch : 1 Loss: 0.000\n","Training Accuracy 100.0 43.28739356994629\n","Epoch : 2 Loss: 0.000\n","Training Accuracy 100.0 43.76031994819641\n","Epoch : 3 Loss: 0.000\n","Training Accuracy 100.0 44.07192420959473\n","Epoch : 4 Loss: 0.000\n","Training Accuracy 100.0 44.449360609054565\n","Epoch : 5 Loss: 0.000\n","Training Accuracy 100.0 44.62438917160034\n","Epoch : 6 Loss: 0.000\n","Training Accuracy 100.0 44.9768705368042\n","Epoch : 7 Loss: 0.000\n","Training Accuracy 100.0 45.30792593955994\n","Epoch : 8 Loss: 0.000\n","Training Accuracy 100.0 45.22724461555481\n","Epoch : 9 Loss: 0.000\n","Training Accuracy 100.0 45.620566606521606\n","Epoch : 10 Loss: 0.000\n","Training Accuracy 100.0 45.61465525627136\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aH0KIfI8EM3R","colab_type":"code","colab":{}},"source":["train_data_full_numpy = []\n","train_labels_full_numpy = []\n","\n","for i in range(len(test_data)):\n","  train_data_full_numpy.append(train_data_full[i].numpy())\n","  train_labels_full_numpy.append(train_labels_full[i].numpy()) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQdg3lMBSZkO","colab_type":"code","colab":{}},"source":["#train_data_full_numpy = np.concatenate([i for i in train_data_full_numpy])\n","#train_labels_full_numpy = np.concatenate([i for i in train_labels_full_numpy])\n","train_svm = []\n","for i in range(len(train_data_full_numpy)):\n","  train_svm.append(np.concatenate(train_data_full_numpy[i])) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPfm7VpKRa5f","colab_type":"code","outputId":"61b3657d-fd8e-40e9-f020-c21ffc1a799a","executionInfo":{"status":"ok","timestamp":1574480185907,"user_tz":300,"elapsed":177722,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["from sklearn.metrics import accuracy_score \n","from sklearn.svm import LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","clf =  OneVsRestClassifier(LinearSVC(random_state=None ,tol=1e-5, loss='squared_hinge', C=100, multi_class='ovr', max_iter=2000))\n","clf.fit(np.asarray(train_data_full_numpy), np.asarray(train_labels_full_numpy))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneVsRestClassifier(estimator=LinearSVC(C=100, class_weight=None, dual=True,\n","                                        fit_intercept=True, intercept_scaling=1,\n","                                        loss='squared_hinge', max_iter=2000,\n","                                        multi_class='ovr', penalty='l2',\n","                                        random_state=None, tol=1e-05,\n","                                        verbose=0),\n","                    n_jobs=None)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"7tfQfXFCVJ3K","colab_type":"code","outputId":"6e1a3154-ef26-43a6-9960-a4e070261f81","executionInfo":{"status":"ok","timestamp":1574480878329,"user_tz":300,"elapsed":7242,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pred_labels = clf.predict(np.asarray(train_data_full_numpy))\n","training_accuracy_svm = accuracy_score(train_labels_full_numpy, pred_labels)\n","print(\"The training accuracy is {:.2f}%\".format(training_accuracy_svm*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The training accuracy is 100.00%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jEtg2RJNTyXI","colab_type":"code","outputId":"c5e83711-accb-4a95-f13d-c95b7204cd8e","executionInfo":{"status":"ok","timestamp":1574480898370,"user_tz":300,"elapsed":4619,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["shuffled_test = np.asarray(test_data_numpy)\n","shuffled_test = np.random.shuffle(shuffled_test)\n","pred_labels = clf.predict(np.asarray(test_data_numpy))\n","testing_accuracy_svm = accuracy_test1(test_labels_numpy, pred_labels) \n","print(\"The testing accuracy is {:.2f}%\".format(testing_accuracy_svm*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The testing accuracy is 73.89%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ip87hPqTYJtr","outputId":"fb5b6077-719f-4d73-a54f-861c7ecda8db","executionInfo":{"status":"ok","timestamp":1574480914101,"user_tz":300,"elapsed":706,"user":{"displayName":"Gautham Ramajayam","photoUrl":"","userId":"03590966262809164737"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Don't hardcode the train and test accuracy\n","print('Training accuracy is %2.3f :' %(100.00), \n","training_accuracy_svm*100)\n","print('Test accuracy is %2.3f :' %(100.00), testing_accuracy_svm*100 )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training accuracy is 100.000 : 100.0\n","Test accuracy is 100.000 : 73.89473684210527\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cL4Y3nHBkwmb"},"source":["## **Problem 4.** Report"]},{"cell_type":"markdown","metadata":{"id":"JbzpDeRRNuHj","colab_type":"text"},"source":["## **Bonus**\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jPrhLzuyN-rr"},"source":["* ##### **Print the size of your training and test data**"]},{"cell_type":"code","metadata":{"id":"-9b7f2RSrXeE","colab_type":"code","colab":{}},"source":["df_76 = df[df.class_id > 25]\n","\n","df_76_train = df_76[df_76.data_usage == 1]\n","df_76_test = df_76[df_76.data_usage == 2]\n","\n","train76_folders = df_76_train.folder.tolist()\n","train76_labels = df_76_train.class_id.tolist()\n","\n","test76_folders = df_76_test.folder.tolist()\n","test76_labels = df_76_test.class_id.tolist()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxNRaniArZu7","colab_type":"code","colab":{}},"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","\n","vgg16_local_76 = torchvision.models.vgg16(True).to(device)\n","del vgg16_local_76.classifier[1:]\n","\n","print(vgg16_local_76.classifier)\n","\n","normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","prep = torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), normalize ])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-IUMmF2lrb1d","colab_type":"code","colab":{}},"source":["#Code to extract Remaining 76 classes\n","\n","\"\"\"\n","import gc\n","import time\n","\n","training76_feats = []\n","training76_labels = []\n","root_dir_imgs = \"./images\"\n","\n","full_time = time.time()\n","\n","for idx in range(len(train76_folders)):\n","        start = time.time()\n","        folder_name = os.path.join(root_dir_imgs, train76_folders[idx])\n","        label = train76_labels[idx]\n","        \n","        files = os.listdir(folder_name)\n","        #print(files)\n","        #for i in range(len(files)):\n","        i = 0\n","        img125 = []\n","        feats25 = torch.empty(0)\n","        while i < 25:\n","          \n","          file_name = os.path.join(folder_name, files[i])\n","            \n","          #img_name = img_name + '/*.jpg' \n","          #image = cv2.imread(file_name)\n","          \n","\n","          #if idx % 25 == 0:\n","          #print(folder_name, label)\n","          gc.collect()\n","          #torch.cuda.empty_cache()\n","\n","          #print(file_name)\n","          img = cv2.imread(file_name)\n","          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","          img = prep(img)\n","          #print(img.size()[0])\n","          img_c = img[ :, (img.shape[1]-224)//2:(img.shape[1]+224)//2, (img.shape[2]-224)//2:(img.shape[2]+224)//2]\n","          img_tl = img[:, :224, :224] \n","          img_tr = img[:, :224, (img.shape[2]-224):]\n","          img_bl = img[:, (img.shape[1]-224):, :224] \n","          img_br = img[:, (img.shape[1]-224):, (img.shape[2]-224):] \n","          #print(len(img_tr[2]), len(img_bl[1]), len(img_br[0]))\n","          #print(img_c.size(), img_tl.size(), img_tr.size(), img_bl.size(), img_br.size())\n","          img125.extend([img_c, img_tl, img_tr, img_bl, img_br])\n","\n","          i += 1\n","\n","        #print(\"img125\", len(img125))\n","\n","        with torch.no_grad():\n","          inputs = torch.stack(img125)\n","          #print(len(inputs))\n","          inputs = inputs.to(device)\n","          #img_feat = self.vgg16(inputs)\n","          \n","          img_feat125 = vgg16_local_76(inputs)\n","          #print(img_feat125.shape)\n","          img_feat125 = img_feat125.cpu().detach().numpy()\n","          #print(inputs.is_cuda, img_feat1.is_cuda, img_c.is_cuda)#, img_feat1.is_cuda)\n","          del inputs\n","          torch.cuda.empty_cache()\n","          #gc.collect()\n","          #img_feat = self.vgg16(img_c)\n","          #print(len(img_feat), len(img_feat[0]),len(img_feat[1]),len(img_feat[2]))\n","\n","        j = 0\n","        feats_video = np.zeros(0)\n","        while j < 25:\n","          feats25_mean = img_feat125[5*j:5*(j+1)]\n","          #print(feats25_mean.shape)\n","          feats25_mean = np.mean(feats25_mean, axis = 0)\n","          #print(\"feats25_mean\", feats25_mean.shape)\n","          if j == 0:\n","            feats_video = feats25_mean            \n","          else:\n","            feats_video = np.concatenate((feats_video, feats25_mean), 0)\n","          j += 1  \n","\n","        del img_feat125  \n","        torch.cuda.empty_cache()\n","        gc.collect()    \n","\n","        #print(\"feats_video\",feats_video.shape)\n","        training76_feats.append(feats_video)  \n","        training76_labels.append(label)\n","\n","        if idx % 100 == 0 and idx != 0:\n","          print(idx)\n","          f = open('training76_feats_labels.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","          pickle.dump([training76_feats, training76_labels], f, -1)          # dump data to f\n","          f.close()\n","\n","\n","        print(\"Folders done:\", len(training76_feats), label, time.time() - start, time.time() - full_time)\n","\n","\"\"\"        \n","       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jda-kW_rjBv","colab_type":"code","colab":{}},"source":["\"\"\"\n","import gc\n","import time\n","import pickle\n","\n","testing76_feats = []\n","testing76_labels = []\n","root_dir_imgs = \"./images\"\n","\n","full_time = time.time()\n","\n","for idx in range(len(test76_folders)):\n","        start = time.time()\n","        folder_name = os.path.join(root_dir_imgs, test76_folders[idx])\n","        label = test76_labels[idx]\n","        \n","        files = os.listdir(folder_name)\n","        #print(files)\n","        #for i in range(len(files)):\n","        i = 0\n","        img125 = []\n","        feats25 = torch.empty(0)\n","        while i < 25:\n","          \n","          file_name = os.path.join(folder_name, files[i])\n","            \n","          #img_name = img_name + '/*.jpg' \n","          #image = cv2.imread(file_name)\n","          \n","\n","          #if idx % 25 == 0:\n","          #print(folder_name, label)\n","          gc.collect()\n","          #torch.cuda.empty_cache()\n","\n","          #print(file_name)\n","          img = cv2.imread(file_name)\n","          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","          img = prep(img)\n","          #print(img.size()[0])\n","          img_c = img[ :, (img.shape[1]-224)//2:(img.shape[1]+224)//2, (img.shape[2]-224)//2:(img.shape[2]+224)//2]\n","          img_tl = img[:, :224, :224] \n","          img_tr = img[:, :224, (img.shape[2]-224):]\n","          img_bl = img[:, (img.shape[1]-224):, :224] \n","          img_br = img[:, (img.shape[1]-224):, (img.shape[2]-224):] \n","          #print(len(img_tr[2]), len(img_bl[1]), len(img_br[0]))\n","          #print(img_c.size(), img_tl.size(), img_tr.size(), img_bl.size(), img_br.size())\n","          img125.extend([img_c, img_tl, img_tr, img_bl, img_br])\n","\n","          i += 1\n","\n","        #print(\"img125\", len(img125))\n","\n","        with torch.no_grad():\n","          inputs = torch.stack(img125)\n","          #print(len(inputs))\n","          inputs = inputs.to(device)\n","          #img_feat = self.vgg16(inputs)\n","          \n","          img_feat125 = vgg16_local_76(inputs)\n","          #print(img_feat125.shape)\n","          img_feat125 = img_feat125.cpu().detach().numpy()\n","          #print(inputs.is_cuda, img_feat1.is_cuda, img_c.is_cuda)#, img_feat1.is_cuda)\n","          del inputs\n","          torch.cuda.empty_cache()\n","          #gc.collect()\n","          #img_feat = self.vgg16(img_c)\n","          #print(len(img_feat), len(img_feat[0]),len(img_feat[1]),len(img_feat[2]))\n","\n","        j = 0\n","        feats_video = np.zeros(0)\n","        while j < 25:\n","          feats25_mean = img_feat125[5*j:5*(j+1)]\n","          #print(feats25_mean.shape)\n","          feats25_mean = np.mean(feats25_mean, axis = 0)\n","          #print(\"feats25_mean\", feats25_mean.shape)\n","          if j == 0:\n","            feats_video = feats25_mean            \n","          else:\n","            feats_video = np.concatenate((feats_video, feats25_mean), 0)\n","          j += 1  \n","\n","        del img_feat125  \n","        torch.cuda.empty_cache()\n","        gc.collect()    \n","\n","        #print(\"feats_video\",feats_video.shape)\n","        testing76_feats.append(feats_video)  \n","        testing76_labels.append(label)\n","\n","        if idx % 100 == 0 and idx != 0:\n","          print(idx)\n","          f = open('testing76_feats_labels.pkl', 'wb')   # Pickle file is newly created where foo1.py is\n","          pickle.dump([testing76_feats, testing76_labels], f, -1)          # dump data to f\n","          f.close()\n","\n","\n","        print(\"Folders done:\", len(testing76_feats),\"/\", len(test76_folders), label, time.time() - start, time.time() - full_time)\n","\n","\"\"\"        \n","       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b8IR6zrwOENz","colab_type":"code","colab":{}},"source":["# Don't hardcode the shape of train and test data\n","print('Shape of training data is :', )\n","print('Shape of test/validation data is :', )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GAakVg8-OE_j"},"source":["* ##### **Modelling and evaluation**"]},{"cell_type":"code","metadata":{"id":"QHbPzkcoObLb","colab_type":"code","colab":{}},"source":["#Write your code for modelling and evaluation"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vb4Wlzw2jYcJ"},"source":["## Submission\n","---\n","**Runnable source code in ipynb file and a pdf report are required**.\n","\n","The report should be of 3 to 4 pages describing what you have done and learned in this homework and report performance of your model. If you have tried multiple methods, please compare your results. If you are using any external code, please cite it in your report. Note that this homework is designed to help you explore and get familiar with the techniques. The final grading will be largely based on your prediction accuracy and the different methods you tried (different architectures and parameters).\n","\n","Please indicate clearly in your report what model you have tried, what techniques you applied to improve the performance and report their accuracies. The report should be concise and include the highlights of your efforts.\n","The naming convention for report is **Surname_Givenname_SBUID_report*.pdf**\n","\n","When submitting your .zip file through blackboard, please\n","-- name your .zip file as **Surname_Givenname_SBUID_hw*.zip**.\n","\n","This zip file should include:\n","```\n","Surname_Givenname_SBUID_hw*\n","        |---Surname_Givenname_SBUID_hw*.ipynb\n","        |---Surname_Givenname_SBUID_hw*.pdf\n","        |---Surname_Givenname_SBUID_report*.pdf\n","```\n","\n","For instance, student Michael Jordan should submit a zip file named \"Jordan_Michael_111134567_hw5.zip\" for homework5 in this structure:\n","```\n","Jordan_Michael_111134567_hw5\n","        |---Jordan_Michael_111134567_hw5.ipynb\n","        |---Jordan_Michael_111134567_hw5.pdf\n","        |---Jordan_Michael_111134567_report*.pdf\n","```\n","\n","The **Surname_Givenname_SBUID_hw*.pdf** should include a **google shared link**. To generate the **google shared link**, first create a folder named **Surname_Givenname_SBUID_hw*** in your Google Drive with your Stony Brook account. \n","\n","Then right click this folder, click ***Get shareable link***, in the People textfield, enter two TA's emails: ***bo.cao.1@stonybrook.edu*** and ***sayontan.ghosh@stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n","\n","Colab has a good feature of version control, you should take advantage of this to save your work properly. However, the timestamp of the submission made in blackboard is the only one that we consider for grading. To be more specific, we will only grade the version of your code right before the timestamp of the submission made in blackboard. \n","\n","You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n","\n","Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work."]}]}